{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPGJiWKEQJFNkomWkOJdyiv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AgusLuigi/retail_demand_analysis/blob/retail_demand_forecast/Standard_DATA_ANALIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA ANALIST"
      ],
      "metadata": {
        "id": "5WX3on7-l5uQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Problemdefinition und Datenerfassung\n",
        " - Was soll mit der Analyse erreicht werden?\n",
        " - Wer sind die Stakeholder und was sind ihre Erwartungen?"
      ],
      "metadata": {
        "id": "iiiAp91pmA6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vereinfachte Informationsausgabe"
      ],
      "metadata": {
        "id": "5kyYIZ04ms7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ 1. Technisch ‚Äì Tiefer gehen als Standard\n",
        "\n",
        "Du hast bereits: Missing Values, Sonderzeichen, Duplikate, Outlier-Check.\n",
        "F√ºr 1 musst du zeigen, dass du:\n",
        "\n",
        "Mehrere Methoden vergleichst:\n",
        "‚Üí z. B. Imputation mit Median und KNN/Regression vergleichen und kurz begr√ºnden.\n",
        "\n",
        "Datenlogik pr√ºfst:\n",
        "‚Üí z. B. unit_price = revenue / nb_sold muss in einem plausiblen Preisbereich liegen.\n",
        "‚Üí Wenn Werte unlogisch sind, kennzeichnen oder korrigieren.\n",
        "\n",
        "Datenqualit√§t dokumentierst:\n",
        "‚Üí Before/After-Tabelle (wie viele Fehler behoben).\n",
        "‚Üí Visuals: Heatmap f√ºr Missing Values, Boxplots f√ºr Ausrei√üer.\n",
        "\n",
        "Plausibilit√§ts-Checks mit Business-Verstand:\n",
        "‚Üí years_as_customer > 60 ‚Üí erkl√§ren (realistisch oder Datenfehler?).\n",
        "\n",
        "üëâ Hier punkten Pr√ºfer: du denkst √ºber die Zahlen hinaus.\n",
        "\n",
        "‚úÖ 2. Business Metrics (KPI) ‚Äì Messen & Baseline\n",
        "\n",
        "‚ÄûInsufficient‚Äú hattest du bei Business Metrics.\n",
        "F√ºr eine 1 brauchst du:\n",
        "\n",
        "Mindestens 2 relevante KPIs definieren (z. B. Conversion Rate, Revenue per Customer, Retention).\n",
        "\n",
        "Baseline-Wert berechnen (z. B. durchschnittliche Conversion Rate = 12,3%).\n",
        "\n",
        "Ziel definieren (z. B. ‚Äû+2% Conversion in 3 Monaten‚Äú).\n",
        "\n",
        "Visualisierung: z. B. Trendplot der Conversion √ºber die Wochen.\n",
        "\n",
        "Handlungsempfehlung: was tun, wenn KPI sinkt/steigt?\n",
        "\n",
        "üëâ Damit zeigst du: Ich mache nicht nur Daten sauber, sondern liefere messbare Business-Werte.\n",
        "\n",
        "‚úÖ 3. Kommunikation ‚Äì Story & Pr√§sentation\n",
        "\n",
        "Viele fallen hier zur√ºck, weil sie nur Code zeigen. F√ºr eine 1 brauchst du:\n",
        "\n",
        "Storytelling-Struktur in deiner Abgabe/Pr√§sentation:\n",
        "\n",
        "Problem (Warum schauen wir auf diese Daten?)\n",
        "\n",
        "Vorgehen (Data Validation, Visualisation, Metrics)\n",
        "\n",
        "Erkenntnisse (Welche Muster gefunden?)\n",
        "\n",
        "Empfehlungen (Was soll das Business tun?)\n",
        "\n",
        "Executive Summary (1 Seite/Folie):\n",
        "\n",
        "3 Haupt-Insights\n",
        "\n",
        "2 KPIs mit Baseline + Ziel\n",
        "\n",
        "1 Handlungsempfehlung\n",
        "\n",
        "Visuals einfach & klar (kein Code-Screenshot, sondern saubere Diagramme mit Titel, Achsen, Takeaway)."
      ],
      "metadata": {
        "id": "ZBDIT4YoFWeM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "üìå To-do-Liste f√ºr dich (auf eine 1 hinarbeiten)\n",
        "\n",
        "Data Validation ausbauen\n",
        "\n",
        "Alternative Imputation (Median vs. KNN).\n",
        "\n",
        "Outlier-Analyse mit Visualisierung (Boxplot, Histogramm).\n",
        "\n",
        "Konsistenzpr√ºfung (Revenue vs nb_sold).\n",
        "\n",
        "Before/After Dokumentation.\n",
        "\n",
        "Business Metrics hinzuf√ºgen\n",
        "\n",
        "2 KPIs definieren (z. B. Umsatz/Kunde, Conversion Rate).\n",
        "\n",
        "Baseline berechnen.\n",
        "\n",
        "Ziel formulieren.\n",
        "\n",
        "Trendvisualisierung.\n",
        "\n",
        "Kommunikation verbessern\n",
        "\n",
        "Ergebnisse nicht nur als Output, sondern als Bericht zusammenfassen.\n",
        "\n",
        "Executive Summary + klare Handlungsempfehlungen."
      ],
      "metadata": {
        "id": "ORvTqihmFugJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Datenbereinigung und -aufbereitung (Data Preprocessing)\n",
        "\n",
        "Behandlung fehlender Werte: Ersetzen, L√∂schen oder Imputieren von Daten.\n",
        "\n",
        "Fehlerbehebung: Korrektur von Tippfehlern, inkonsistenten Werten oder strukturellen Fehlern.\n",
        "\n",
        "Formatierung: Umwandlung von Datentypen, z.B. von Text in Zahlen oder Datumsformate.\n",
        "\n",
        "Duplikatentfernung: Identifizierung und L√∂schung doppelter Eintr√§ge."
      ],
      "metadata": {
        "id": "JXpDaYZQmtww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SEMANTISCHE ERKENUNG + Daten deteils + BEREINIGUNGS + ML data qualyty + ML vorbereitung"
      ],
      "metadata": {
        "id": "4hfe7dL-kADb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import re\n",
        "from typing import Dict, Any, List, Callable, Union\n",
        "\n",
        "# ======================================================\n",
        "# Erl√§uterung des Skripts\n",
        "# ======================================================\n",
        "\"\"\"\n",
        "Dieses Skript ist ein optimierter Datenqualit√§ts-Workflow, der einen umfassenden\n",
        "Bericht √ºber einen Pandas DataFrame erstellt. Er analysiert die Spalten\n",
        "basierend auf ihrem Inhalt und Namen, um den semantischen Typ zu erkennen (z. B.\n",
        "Datum, Text, Integer, W√§hrung). Auf dieser Grundlage identifiziert er potenzielle\n",
        "Datenprobleme und generiert automatisch Vorschl√§ge zur Bereinigung und\n",
        "Vorverarbeitung der Daten, was besonders f√ºr Machine-Learning-Anwendungen\n",
        "n√ºtzlich ist.\n",
        "\n",
        "Der generierte Bericht ist in vier logische Module unterteilt:\n",
        "\n",
        "MODUL 1: ALLGEMEINE √úBERSICHT\n",
        "- Zeigt grundlegende Informationen wie den semantischen Typ, den urspr√ºnglichen\n",
        "  Datentyp, die Anzahl fehlender Werte und die Anzahl der einzigartigen Werte\n",
        "  f√ºr jede Spalte. Es bietet einen schnellen √úberblick √ºber die Struktur und den\n",
        "  Zustand Ihrer Daten.\n",
        "\n",
        "MODUL 2: STATISTISCHE KENNZAHLEN\n",
        "- Stellt statistische Kennzahlen wie Minimum, Quartile (25%, 50%, 75%) und\n",
        "  Maximum f√ºr alle numerischen Spalten bereit. Dies hilft, die Verteilung der\n",
        "  Daten besser zu verstehen und erste Anomalien zu erkennen.\n",
        "\n",
        "MODUL 3: PROBLEME & BEREINIGUNGSVORSCHL√ÑGE\n",
        "- Identifiziert spezifische Probleme wie fehlende Werte oder Text-Inkonsistenzen.\n",
        "- Basierend auf diesen Problemen generiert das Skript einen Bereinigungscode zum\n",
        "  Kopieren. Die Vorschl√§ge folgen einer logischen Reihenfolge:\n",
        "  1. Typkonvertierung: Zum Beispiel die Umwandlung von Strings in numerische oder\n",
        "     Datumsformate.\n",
        "  2. Missing-Value-Imputation: Fehlende Werte werden durch intelligente Methoden\n",
        "     gef√ºllt. F√ºr Textspalten wird versucht, einen passenden Modus basierend auf\n",
        "     verwandten Spalten zu finden, w√§hrend f√ºr numerische Spalten der Median\n",
        "     verwendet wird.\n",
        "\n",
        "MODUL 4: ML-RELEVANTE ANALYSE\n",
        "- Untersucht die Verteilung numerischer Spalten auf Schiefe (Skewness) und\n",
        "  identifiziert Ausrei√üer mittels der IQR-Methode. Es analysiert auch hohe\n",
        "  Korrelationen zwischen den Spalten.\n",
        "- Das Modul schl√§gt spezifische Vorverarbeitungsschritte vor, wie das Capping\n",
        "  von Ausrei√üern oder Log-Transformationen, um die Daten f√ºr Machine-Learning-Modelle\n",
        "  vorzubereiten.\n",
        "\n",
        "Args:\n",
        "    df (pd.DataFrame): Der zu analysierende DataFrame.\n",
        "\n",
        "Returns:\n",
        "    None: Der Bericht wird direkt auf der Konsole ausgegeben.\n",
        "\"\"\"\n",
        "# ======================================================\n",
        "\n",
        "# Abk√ºrzungs- und Tippfehler-W√∂rterbuch f√ºr die Textbereinigung\n",
        "TEXT_CORRECTION_MAP = {\n",
        "    'str.': 'Stra√üe',\n",
        "    'str': 'Stra√üe',\n",
        "    'st.': 'Sankt',\n",
        "    'z.b.': 'zum Beispiel',\n",
        "    'usw.': 'und so weiter',\n",
        "    'z.t.': 'zum Teil',\n",
        "    'ltd.': 'limited',\n",
        "    'inc.': 'incorporated',\n",
        "    'corp.': 'corporation',\n",
        "    'gmbh': 'GmbH',\n",
        "    'ag': 'AG',\n",
        "    'yes': 'ja',\n",
        "    'no': 'nein'\n",
        "}\n",
        "\n",
        "def _correct_spelling_and_expand_abbr(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Korrigiert h√§ufige Schreibfehler und erweitert Abk√ºrzungen in einem Text.\n",
        "\n",
        "    Args:\n",
        "        text (str): Der Eingabetext, der korrigiert werden soll.\n",
        "\n",
        "    Returns:\n",
        "        str: Der korrigierte Text.\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    text_lower = text.lower().strip()\n",
        "    words = text_lower.split()\n",
        "    corrected_words = [TEXT_CORRECTION_MAP.get(word, word) for word in words]\n",
        "    return ' '.join(corrected_words)\n",
        "\n",
        "def generate_cleaning_code(column: str, semantic_type: str) -> str:\n",
        "    \"\"\"\n",
        "    Generiert einen Python-Code-Vorschlag zur Datenbereinigung basierend auf dem\n",
        "    erkannten semantischen Typ.\n",
        "\n",
        "    Args:\n",
        "        column (str): Der Name der zu bereinigenden Spalte.\n",
        "        semantic_type (str): Der erkannte semantische Typ der Spalte.\n",
        "\n",
        "    Returns:\n",
        "        str: Eine Code-Zeile zur Bereinigung der Spalte.\n",
        "    \"\"\"\n",
        "    if semantic_type == 'Datum/Zeit':\n",
        "        return f\"df['{column}'] = pd.to_datetime(df['{column}'], errors='coerce')\"\n",
        "    elif semantic_type == 'ID':\n",
        "        return f\"df['{column}'] = df['{column}'].astype('object')\"\n",
        "    elif semantic_type == 'Boolean':\n",
        "        return f\"df['{column}'] = df['{column}'].astype(bool)\"\n",
        "    elif semantic_type == 'Integer':\n",
        "        return f\"df['{column}'] = pd.to_numeric(df['{column}'], errors='coerce').astype('Int64')\"\n",
        "    elif 'Float' in semantic_type:\n",
        "        return f\"df['{column}'] = pd.to_numeric(df['{column}'].astype(str).str.replace(',', '.'), errors='coerce')\"\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def _fill_missing_with_pattern_mode(df: pd.DataFrame, target_col: str, group_col: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    F√ºllt fehlende Werte in einer Spalte basierend auf dem h√§ufigsten Wert\n",
        "    innerhalb einer Gruppe (Modus).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Der zu verarbeitende DataFrame.\n",
        "        target_col (str): Die Spalte mit fehlenden Werten.\n",
        "        group_col (str): Die Spalte, nach der gruppiert werden soll.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Der DataFrame mit gef√ºllten fehlenden Werten.\n",
        "    \"\"\"\n",
        "    print(f\"    - F√ºlle fehlende Werte in '{target_col}' basierend auf '{group_col}'\")\n",
        "\n",
        "    if target_col not in df.columns or group_col not in df.columns:\n",
        "        print(f\"      FEHLER: Spalten '{target_col}' oder '{group_col}' nicht gefunden. √úberspringe.\")\n",
        "        return df\n",
        "\n",
        "    mode_by_group = df.groupby(group_col)[target_col].apply(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
        "    mode_by_group.name = 'pattern_mode'\n",
        "\n",
        "    df = df.merge(mode_by_group, on=group_col, how='left', suffixes=('', '_pattern'))\n",
        "\n",
        "    df[target_col] = df[target_col].fillna(df['pattern_mode'])\n",
        "    df = df.drop(columns='pattern_mode')\n",
        "\n",
        "    global_mode = df[target_col].mode()\n",
        "    if not global_mode.empty:\n",
        "        df[target_col] = df[target_col].fillna(global_mode.iloc[0])\n",
        "\n",
        "    return df\n",
        "\n",
        "def _find_best_grouping_column(df: pd.DataFrame, target_col: str, df_analysis: pd.DataFrame) -> Union[str, None]:\n",
        "    \"\"\"\n",
        "    Findet die beste Gruppierungsspalte f√ºr eine Zielspalte, basierend auf dem\n",
        "    statistischen Abh√§ngigkeits-Score.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Der zu verarbeitende DataFrame.\n",
        "        target_col (str): Die Spalte, die gef√ºllt werden soll.\n",
        "        df_analysis (pd.DataFrame): Der DataFrame mit der semantischen Analyse.\n",
        "\n",
        "    Returns:\n",
        "        Union[str, None]: Der Name der besten Gruppierungsspalte oder None.\n",
        "    \"\"\"\n",
        "    text_columns = [col for col, sem_type in zip(df_analysis['Spalte'], df_analysis['Semantischer Typ'])\n",
        "                    if 'Text' in sem_type and col != target_col]\n",
        "\n",
        "    if not text_columns:\n",
        "        return None\n",
        "\n",
        "    best_group_col = None\n",
        "    best_score = float('inf')\n",
        "\n",
        "    df_clean = df.dropna(subset=[target_col])\n",
        "    if df_clean.empty:\n",
        "        return None\n",
        "\n",
        "    for group_col in text_columns:\n",
        "        if group_col in df_clean.columns:\n",
        "            dependency_score = df_clean.groupby(group_col)[target_col].nunique().mean()\n",
        "            if dependency_score < best_score:\n",
        "                best_score = dependency_score\n",
        "                best_group_col = group_col\n",
        "\n",
        "    if best_score < 1.2:\n",
        "        return best_group_col\n",
        "\n",
        "    return None\n",
        "\n",
        "def analyze_semantic_type_v3(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Analysiert die semantischen Datentypen der Spalten in einem DataFrame mit\n",
        "    einer angepassten Logik, bei der Spaltenname und Inhalt f√ºr die\n",
        "    semantische Klassifizierung √ºbereinstimmen m√ºssen.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Der zu analysierende DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Ein DataFrame mit den Spaltennamen, urspr√ºnglichen Datentypen\n",
        "                      und den erkannten semantischen Typen.\n",
        "    \"\"\"\n",
        "    SEMANTIC_HINTS_PRIORITY: Dict[str, Dict[str, Union[set, Callable]]] = {\n",
        "        'ID': {\n",
        "            'keywords': {'id', 'session_id', 'trip_id', 'user_id', 'unique_id', 'kundennummer', 'bestellnr', 'order_id', 'artikelnummer'},\n",
        "            'validation_func': lambda s: ((s.dropna().astype(str).apply(len) >= 5).any())\n",
        "        },\n",
        "        'Datum/Zeit': {\n",
        "            'keywords': {'week','datum', 'zeit', 'date', 'time', 'start', 'end', 'birthdate', 'signup_date', 'check_in', 'check_out', 'departure', 'return', 'geburtstag', 'timestamp', 'creation_date', 'modified_date', 'erstellt'},\n",
        "            'validation_func': lambda s: (pd.to_datetime(s.dropna(), errors='coerce').notna().all() or (s.dropna().astype(str).str.contains(r'[-_/]', na=False).any() and s.dropna().astype(str).str.contains(r'\\d{4}', na=False).any()))\n",
        "        },\n",
        "        'Geometrisch': {\n",
        "            'keywords': {'geom', 'geometry', 'shape', 'wkt', 'geojson', 'coordinates', 'location_data'},\n",
        "            'validation_func': lambda s: (s.dropna().astype(str).str.contains(r'^(POINT|LINESTRING|POLYGON|MULTIPOINT|MULTILINESTRING|MULTIPOLYGON)\\s*\\(', regex=True, na=False).any() or s.dropna().astype(str).str.contains(r'{\"type\":\\s*\"(Point|LineString|Polygon|MultiPoint|MultiLineString|MultiPolygon)\"', regex=True, na=False).any())\n",
        "        },\n",
        "    }\n",
        "    SEMANTIC_HINTS_TEXT: Dict[str, Dict[str, Union[set, Callable]]] = {\n",
        "        'Text (Kategorisch)': {\n",
        "            'keywords': {'city', 'country', 'l√§nder', 'region', 'state', 'bundesland', 'zip', 'plz'},\n",
        "            'validation_func': lambda s: s.dropna().nunique() >= 2 and (pd.api.types.is_string_dtype(s.dropna()) or isinstance(s.dropna().dtype, pd.CategoricalDtype))\n",
        "        },\n",
        "        'Text (Gender)': {\n",
        "            'keywords': {'geschlecht', 'typ', 'category', 'art', 'gender','method'},\n",
        "            'validation_func': lambda s: s.dropna().nunique() >= 2 and (pd.api.types.is_string_dtype(s.dropna()) or isinstance(s.dropna().dtype, pd.CategoricalDtype))\n",
        "        },\n",
        "        'Text (object)': {\n",
        "            'keywords': {'airport', 'destination', 'origin', 'heimat', 'status','class'},\n",
        "            'validation_func': lambda s: s.dropna().nunique() >= 2 and (pd.api.types.is_string_dtype(s.dropna()) or isinstance(s.dropna().dtype, pd.CategoricalDtype))\n",
        "        },\n",
        "        'Text (Freitext)': {\n",
        "            'keywords': {'name', 'hotel', 'airline', 'beschreibung', 'kommentar', 'nachricht', 'adresse'},\n",
        "            'validation_func': lambda s: pd.api.types.is_string_dtype(s.dropna()) or isinstance(s.dropna().dtype, pd.CategoricalDtype)\n",
        "        },\n",
        "    }\n",
        "    SEMANTIC_HINTS_NUMERIC: Dict[str, Dict[str, Union[set, Callable]]] = {\n",
        "        'Boolean': {\n",
        "            'keywords': {'boolean', 'bool', 'booked', 'married', 'cancellation', 'children','discount'},\n",
        "            'validation_func': lambda s: (s.dropna().nunique() == 2) and (pd.api.types.is_bool_dtype(s.dropna()) or set(s.dropna().astype(str).str.lower().str.strip().unique()).issubset({'true', 'false', '1', '0', 'ja', 'nein', 'yes', 'no', 't', 'f', 'wahr', 'falsch'}))\n",
        "        },\n",
        "        'Float (Geografisch)': {\n",
        "            'keywords': {'lat', 'lon', 'latitude', 'longitude'},\n",
        "            'validation_func': lambda s: pd.to_numeric(s, errors='coerce').notna().all() and pd.api.types.is_float_dtype(s)\n",
        "        },\n",
        "        'Float (Prozentsatz)': {\n",
        "            'keywords': {'percent', 'pct', 'rate', 'discount', '%'},\n",
        "            'validation_func': lambda s: (pd.to_numeric(s.dropna().astype(str).str.replace('%', ''), errors='coerce').dropna().between(0, 1).all() or pd.to_numeric(s.dropna().astype(str).str.replace('%', ''), errors='coerce').dropna().between(0, 100).all()) or pd.to_numeric(s.dropna().astype(str).str.replace('%', ''), errors='coerce').notna().all() and s.dropna().astype(str).str.replace('%', '').str.replace(',', '.').str.match(r'^\\d{1,3}(\\.\\d{1,3})?$').all()\n",
        "        },\n",
        "        'Float (Waehrung)': {\n",
        "            'keywords': {'preis','price', 'kosten', 'betrag', 'revenue', 'dollar', 'euro', 'yen', 'usd', 'eur', 'fare','chf', 'gbp', 'sek', 'jpy', '‚Ç¨', '¬£', '$'},\n",
        "            'validation_func': lambda s: (pd.api.types.is_numeric_dtype(s.dropna()) or pd.to_numeric(s.dropna().astype(str).str.replace(',', '.'), errors='coerce').notna().all()) and s.dropna().nunique() > 2\n",
        "        },\n",
        "        'Float (Masse)': {\n",
        "            'keywords': {'circularity', 'distance_circularity', 'radius_ratio', 'max.length_aspect_ratio', 'scaled_radius_of_gyration', 'scaled_radius_of_gyration.1', 'pr.axis_aspect_ratio', 'pr.axis_rectangularity', 'scaled_variance', 'scaled_variance.1', 'scatter_ratio', 'elongatedness', 'skewness_about', 'skewness_about.1', 'skewness_about.2', 'gewicht', 'length', 'width', 'height', 'weight'},\n",
        "            'validation_func': lambda s: pd.api.types.is_numeric_dtype(s) and s.dropna().nunique() > 2\n",
        "        },\n",
        "        'Integer': {\n",
        "            'keywords': {'nb','anzahl', 'menge', 'stueck', 'stk', 'count', 'qty', 'seats', 'rooms', 'nights', 'bags', 'clicks', 'nummer', 'nr', 'quantity', 'val', 'rating','years_as_customer'},\n",
        "            'validation_func': lambda s: pd.to_numeric(s.dropna(), errors='coerce').notna().all() and (pd.to_numeric(s.dropna(), errors='coerce').dropna().apply(lambda x: x.is_integer() if isinstance(x, float) else True).all())\n",
        "        }\n",
        "    }\n",
        "    results: List[Dict[str, str]] = []\n",
        "    hint_categories = [SEMANTIC_HINTS_PRIORITY, SEMANTIC_HINTS_TEXT, SEMANTIC_HINTS_NUMERIC]\n",
        "    SEMANTIC_HINTS_NUMERIC_ORDERED: List[str] = ['Boolean', 'Float (Geografisch)', 'Float (Prozentsatz)', 'Float (Waehrung)','Float (Masse)', 'Integer']\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
        "        warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "        for column in df.columns:\n",
        "            original_dtype: str = str(df[column].dtype)\n",
        "            semantic_type: str = original_dtype\n",
        "            column_lower: str = column.lower()\n",
        "            found_match: bool = False\n",
        "\n",
        "            for hint_group in hint_categories:\n",
        "                if found_match:\n",
        "                    break\n",
        "                if hint_group is SEMANTIC_HINTS_NUMERIC:\n",
        "                    for sem_type in SEMANTIC_HINTS_NUMERIC_ORDERED:\n",
        "                        hints = hint_group[sem_type]\n",
        "                        name_match = any(keyword in column_lower for keyword in hints['keywords'])\n",
        "                        content_valid = False\n",
        "                        try:\n",
        "                            content_valid = hints['validation_func'](df[column])\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                        if name_match and content_valid:\n",
        "                            semantic_type = sem_type\n",
        "                            found_match = True\n",
        "                            break\n",
        "                else:\n",
        "                    for sem_type, hints in hint_group.items():\n",
        "                        name_match = any(keyword in column_lower for keyword in hints['keywords'])\n",
        "                        content_valid = False\n",
        "                        try:\n",
        "                            content_valid = hints['validation_func'](df[column])\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                        if name_match and content_valid:\n",
        "                            semantic_type = sem_type\n",
        "                            found_match = True\n",
        "                            break\n",
        "\n",
        "            cleaning_code = generate_cleaning_code(column, semantic_type)\n",
        "            results.append({\n",
        "                'Spalte': column,\n",
        "                'Urspr√ºnglicher Datentyp': original_dtype,\n",
        "                'Semantischer Typ': semantic_type,\n",
        "                'Bereinigungscode': cleaning_code\n",
        "            })\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def _find_semantic_groups(df_analysis: pd.DataFrame) -> List[str]:\n",
        "    \"\"\"\n",
        "    Findet Spalten, die semantisch zusammengeh√∂ren (z. B. 'Gewicht L√§nge' und\n",
        "    'Gewicht Breite'), und generiert entsprechende Feature-Engineering-Vorschl√§ge.\n",
        "\n",
        "    Args:\n",
        "        df_analysis (pd.DataFrame): Der DataFrame mit der semantischen Analyse.\n",
        "\n",
        "    Returns:\n",
        "        List[str]: Eine Liste von Code-Snippets f√ºr das Feature Engineering.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"--- Analysiere Spalten f√ºr semantische Gruppen ---\")\n",
        "    ml_snippets = []\n",
        "    groups = {}\n",
        "\n",
        "    for _, row in df_analysis.iterrows():\n",
        "        column = row['Spalte']\n",
        "        semantic_type = row['Semantischer Typ']\n",
        "\n",
        "        # Nur numerische Spalten mit verwandten Namen ber√ºcksichtigen\n",
        "        if 'Float' in semantic_type or 'Integer' in semantic_type:\n",
        "            # Einfache Tokenisierung und Normalisierung des Spaltennamens\n",
        "            tokens = re.split(r'[\\s\\._\\-]', column.lower())\n",
        "\n",
        "            # Schl√ºsselw√∂rter f√ºr L√§nge, Breite, H√∂he etc.\n",
        "            if any(dim_token in tokens for dim_token in ['l√§nge', 'breite', 'h√∂he', 'length', 'width', 'height']):\n",
        "                # Findet den Pr√§fix (z.B. 'gewicht', 'preis')\n",
        "                prefix = next((t for t in tokens if t not in ['l√§nge', 'breite', 'h√∂he', 'length', 'width', 'height']), None)\n",
        "                if prefix:\n",
        "                    if prefix not in groups:\n",
        "                        groups[prefix] = []\n",
        "                    groups[prefix].append(column)\n",
        "\n",
        "    if groups:\n",
        "        print(\"Folgende semantische Spaltengruppen wurden identifiziert:\")\n",
        "        for prefix, cols in groups.items():\n",
        "            if len(cols) > 1:\n",
        "                print(f\"- Gruppe '{prefix}': {cols}\")\n",
        "\n",
        "                # Generiere Feature-Engineering-Code\n",
        "                if len(cols) == 2:\n",
        "                    col1 = cols[0]\n",
        "                    col2 = cols[1]\n",
        "                    new_col_name = f\"{prefix}_fl√§che\"\n",
        "                    ml_snippets.append(f\"# Erstelle neues Merkmal '{new_col_name}' aus '{col1}' und '{col2}'\")\n",
        "                    ml_snippets.append(f\"df['{new_col_name}'] = df['{col1}'] * df['{col2}']\")\n",
        "                elif len(cols) == 3:\n",
        "                    col1 = cols[0]\n",
        "                    col2 = cols[1]\n",
        "                    col3 = cols[2]\n",
        "                    new_col_name = f\"{prefix}_volumen\"\n",
        "                    ml_snippets.append(f\"# Erstelle neues Merkmal '{new_col_name}' aus '{col1}', '{col2}' und '{col3}'\")\n",
        "                    ml_snippets.append(f\"df['{new_col_name}'] = df['{col1}'] * df['{col2}'] * df['{col3}']\")\n",
        "    else:\n",
        "        print(\"Keine relevanten semantischen Spaltengruppen gefunden.\")\n",
        "\n",
        "    return ml_snippets\n",
        "\n",
        "\n",
        "def bereinige(df: pd.DataFrame, cleaning_snippets: List[str], ml_snippets: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    F√ºhrt die gesammelten Bereinigungs- und ML-Vorbereitungsschritte aus,\n",
        "    wobei die Bereinigung vor der ML-Vorbereitung angewendet wird.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Der zu bereinigende DataFrame.\n",
        "        cleaning_snippets (List[str]): Eine Liste von allgemeinen Bereinigungssnippets.\n",
        "        ml_snippets (List[str]): Eine Liste von ML-Vorbereitungssnippets.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Der bereinigte DataFrame.\n",
        "    \"\"\"\n",
        "    df_temp = df.copy()\n",
        "\n",
        "    exec_globals = {\n",
        "        'pd': pd,\n",
        "        'np': np,\n",
        "        '_correct_spelling_and_expand_abbr': _correct_spelling_and_expand_abbr,\n",
        "        '_fill_missing_with_pattern_mode': _fill_missing_with_pattern_mode\n",
        "    }\n",
        "\n",
        "    print(\"--- F√ºhre allgemeine Bereinigungsschritte aus ---\")\n",
        "    for snippet in sorted(list(set(cleaning_snippets))):\n",
        "        try:\n",
        "            exec(snippet, exec_globals, {'df': df_temp})\n",
        "        except Exception as e:\n",
        "            print(f\"Fehler beim Ausf√ºhren des Bereinigungsschritts: '{snippet}' - {e}\")\n",
        "\n",
        "    print(\"\\n--- F√ºhre ML-Vorbereitungsschritte aus ---\")\n",
        "    for snippet in sorted(list(set(ml_snippets))):\n",
        "        try:\n",
        "            exec(snippet, exec_globals, {'df': df_temp})\n",
        "        except Exception as e:\n",
        "            print(f\"Fehler beim Ausf√ºhren des ML-Schritts: '{snippet}' - {e}\")\n",
        "\n",
        "    return df_temp\n",
        "\n",
        "def muster_df(df: pd.DataFrame) -> (List[str], List[str]):\n",
        "    \"\"\"\n",
        "    F√ºhrt den gesamten Datenqualit√§ts-Workflow aus:\n",
        "    1. Analysiert semantische Datentypen.\n",
        "    2. Erstellt einen detaillierten Bericht mit Problemen und statistischen Werten.\n",
        "    3. Gibt Bereinigungs- und ML-Vorbereitungscodes aus.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Der zu analysierende DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[List[str], List[str]]: Zwei Listen mit den Bereinigungs-\n",
        "        und ML-Vorbereitungssnippets.\n",
        "    \"\"\"\n",
        "    df_analysis = analyze_semantic_type_v3(df)\n",
        "\n",
        "    general_report_data = []\n",
        "    statistical_table_data = []\n",
        "    problems_report_data = []\n",
        "    cleaning_snippets = []\n",
        "    visual_proofs = []\n",
        "\n",
        "    ml_report_data = []\n",
        "    ml_cleaning_snippets = []\n",
        "\n",
        "    # Dynamische Bestimmung der numerischen und Text-Spalten basierend auf der Analyse\n",
        "    # Hier werden alle Spalten mit numerischen semantischen Typen gesammelt\n",
        "    numeric_semantic_types = {t for t in df_analysis['Semantischer Typ'] if 'Float' in t or 'Integer' in t or 'Boolean' in t}\n",
        "    # Hier werden alle Spalten mit Text-semantischen Typen gesammelt\n",
        "    text_semantic_types = {t for t in df_analysis['Semantischer Typ'] if 'Text' in t}\n",
        "    # Die Gesamtliste der relevanten Typen f√ºr die Bereinigung\n",
        "    relevant_semantic_types = numeric_semantic_types.union(text_semantic_types)\n",
        "\n",
        "    # PHASE 1: DATENERFASSUNG & ANALYSE\n",
        "    for _, row in df_analysis.iterrows():\n",
        "        column = row['Spalte']\n",
        "        semantic_type = row['Semantischer Typ']\n",
        "        original_dtype = row['Urspr√ºnglicher Datentyp']\n",
        "        series = df[column]\n",
        "\n",
        "        missing_count = series.isnull().sum()\n",
        "        missing_percent = round((missing_count / len(series)) * 100, 2)\n",
        "        unique_values = series.nunique()\n",
        "        general_report_data.append({\n",
        "            'Spalte': column,\n",
        "            'Semantischer Typ': semantic_type,\n",
        "            'Urspr√ºnglicher Datentyp': original_dtype,\n",
        "            'Fehlende Werte (%)': missing_percent,\n",
        "            'Fehlende Werte (Anzahl)': missing_count,\n",
        "            'Einzigartige Werte': unique_values\n",
        "        })\n",
        "\n",
        "        # Dynamische Auswahl: Ber√ºcksichtigt alle numerischen semantischen Typen\n",
        "        if semantic_type in numeric_semantic_types:\n",
        "            try:\n",
        "                numeric_series = pd.to_numeric(series, errors='coerce').dropna()\n",
        "                if not numeric_series.empty:\n",
        "                    q1, median, q3 = numeric_series.quantile([0.25, 0.5, 0.75])\n",
        "                    min_val = numeric_series.min()\n",
        "                    max_val = numeric_series.max()\n",
        "\n",
        "                    statistical_table_data.append({\n",
        "                        'Spalte': column,\n",
        "                        'Min': round(min_val, 2),\n",
        "                        '25% (Q1)': round(q1, 2),\n",
        "                        'Median': round(median, 2),\n",
        "                        '75% (Q3)': round(q3, 2),\n",
        "                        'Max': round(max_val, 2)\n",
        "                    })\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        problems = []\n",
        "        is_string_like = pd.api.types.is_string_dtype(series) or pd.api.types.is_object_dtype(series)\n",
        "\n",
        "        if missing_count > 0:\n",
        "            problems.append('Fehlende Werte')\n",
        "        if is_string_like:\n",
        "            text_series = series.dropna().astype(str)\n",
        "            if not text_series.equals(text_series.str.strip().str.lower()):\n",
        "                problems.append('Text-Inkonsistenzen')\n",
        "\n",
        "        if problems:\n",
        "            problems_report_data.append({\n",
        "                'Spalte': column,\n",
        "                'Probleme': ', '.join(problems)\n",
        "            })\n",
        "\n",
        "        # Dynamische Auswahl: Ber√ºcksichtigt alle relevanten Typen\n",
        "        if semantic_type in relevant_semantic_types:\n",
        "            snippets_for_col = []\n",
        "\n",
        "            if 'Text' in semantic_type:\n",
        "                snippets_for_col.append(f\"df['{column}'] = df['{column}'].astype(str).str.lower().str.strip()\")\n",
        "                snippets_for_col.append(f\"df['{column}'] = df['{column}'].apply(_correct_spelling_and_expand_abbr)\")\n",
        "            elif 'Float' in semantic_type or 'Integer' in semantic_type:\n",
        "                snippets_for_col.append(generate_cleaning_code(column, semantic_type))\n",
        "\n",
        "            if missing_count > 0:\n",
        "                if 'Text' in semantic_type:\n",
        "                    group_col = _find_best_grouping_column(df, column, df_analysis)\n",
        "                    if group_col:\n",
        "                        snippets_for_col.append(f\"df = _fill_missing_with_pattern_mode(df, '{column}', '{group_col}')\")\n",
        "                        grouped_mode = df.groupby(group_col)[column].apply(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
        "                        visual_df = pd.DataFrame({\n",
        "                            'Gruppierung': df[group_col].dropna().unique(),\n",
        "                            'Erkannter Modus': [grouped_mode.loc[g] for g in df[group_col].dropna().unique()]\n",
        "                        })\n",
        "                        visual_df = visual_df.head(5)\n",
        "                        visual_proofs.append({'target': column, 'group': group_col, 'data': visual_df})\n",
        "                    else:\n",
        "                        snippets_for_col.append(f\"df['{column}'] = df['{column}'].fillna(df['{column}'].mode()[0])\")\n",
        "                elif pd.api.types.is_numeric_dtype(series):\n",
        "                    median_val = series.median()\n",
        "                    if pd.notna(median_val):\n",
        "                        snippets_for_col.append(f\"df['{column}'] = df['{column}'].fillna(df['{column}'].median())\")\n",
        "                    else:\n",
        "                        snippets_for_col.append(f\"df['{column}'] = df['{column}'].fillna(0)\")\n",
        "\n",
        "            cleaning_snippets.extend([s for s in snippets_for_col if s])\n",
        "\n",
        "        # Dynamische Auswahl: Ber√ºcksichtigt alle numerischen semantischen Typen\n",
        "        if semantic_type in numeric_semantic_types:\n",
        "            try:\n",
        "                numeric_series = pd.to_numeric(series, errors='coerce').dropna()\n",
        "                if not numeric_series.empty:\n",
        "                    skewness = round(numeric_series.skew(), 2)\n",
        "\n",
        "                    Q1 = numeric_series.quantile(0.25)\n",
        "                    Q3 = numeric_series.quantile(0.75)\n",
        "                    IQR = Q3 - Q1\n",
        "                    lower_bound = Q1 - 1.5 * IQR\n",
        "                    upper_bound = Q3 + 1.5 * IQR\n",
        "                    outliers_count = ((numeric_series < lower_bound) | (numeric_series > upper_bound)).sum()\n",
        "\n",
        "                    ml_report_data.append({\n",
        "                        'Spalte': column,\n",
        "                        'Skewness': skewness,\n",
        "                        'Ausrei√üer (IQR-Methode)': outliers_count\n",
        "                    })\n",
        "\n",
        "                    if outliers_count > 0:\n",
        "                        ml_cleaning_snippets.append(f\"df['{column}'] = df['{column}'].clip(lower=df['{column}'].quantile(0.05), upper=df['{column}'].quantile(0.95)) # Ausrei√üer cappen\")\n",
        "                    if skewness > 1 or skewness < -1:\n",
        "                        ml_cleaning_snippets.append(f\"df['{column}'] = np.log1p(df['{column}']) # Log-Transformation zur Reduzierung der Schiefe\")\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    numeric_df = df.select_dtypes(include=np.number)\n",
        "    corr_report = []\n",
        "    if not numeric_df.empty:\n",
        "        corr_matrix = numeric_df.corr().round(2)\n",
        "        for col1 in corr_matrix.columns:\n",
        "            for col2 in corr_matrix.columns:\n",
        "                if col1 != col2 and abs(corr_matrix.loc[col1, col2]) > 0.7:\n",
        "                    if (col2, col1) not in [item['Paar'] for item in corr_report]:\n",
        "                        corr_report.append({\n",
        "                            'Paar': (col1, col2),\n",
        "                            'Korrelation': corr_matrix.loc[col1, col2]\n",
        "                        })\n",
        "\n",
        "    # NEU: Semantische Gruppierung und Feature Engineering\n",
        "    ml_feature_snippets = _find_semantic_groups(df_analysis)\n",
        "    ml_cleaning_snippets.extend(ml_feature_snippets)\n",
        "\n",
        "\n",
        "    # PHASE 2: AUSGABE DER MODULE\n",
        "    # ====== MODUL 1: ALLGEMEINE √úBERSICHT ======\n",
        "    print('*' * 10, 'MODUL 1: ALLGEMEINE √úBERSICHT', '*' * 10)\n",
        "    general_report_df = pd.DataFrame(general_report_data)\n",
        "    print(general_report_df.to_string())\n",
        "    print('*' * 50)\n",
        "    # =============================\n",
        "\n",
        "    # ====== MODUL 2: STATISTISCHE KENNZAHLEN ======\n",
        "    print('*' * 10, 'MODUL 2: STATISTISCHE KENNZAHLEN', '*' * 10)\n",
        "    if statistical_table_data:\n",
        "        statistical_table_df = pd.DataFrame(statistical_table_data)\n",
        "        print(statistical_table_df.to_string(index=False))\n",
        "    else:\n",
        "        print(\"Keine statistischen Kennzahlen f√ºr die gew√§hlten Spaltentypen vorhanden.\")\n",
        "    print('*' * 50)\n",
        "    # =============================\n",
        "\n",
        "    # ====== MODUL 3: PROBLEME & BEREINIGUNGSVORSCHL√ÑGE ======\n",
        "    print('*' * 10, 'MODUL 3: PROBLEME & BEREINIGUNGSVORSCHL√ÑGE', '*' * 10)\n",
        "    if problems_report_data:\n",
        "        problems_df = pd.DataFrame(problems_report_data)\n",
        "        print(problems_df.to_string())\n",
        "        if visual_proofs:\n",
        "            print(\"\\n\" + \"--- Visueller Nachweis der Gruppierung f√ºr Textbereinigung ---\")\n",
        "            for proof in visual_proofs:\n",
        "                print(f\"Beispiel f√ºr '{proof['target']}' (wird gef√ºllt) basierend auf '{proof['group']}'\")\n",
        "                print(proof['data'].to_string(index=False))\n",
        "                print(\"-\" * 50)\n",
        "        print(\"\\n\" + \"Allgemeine Bereinigungsvorschl√§ge zum Kopieren:\")\n",
        "        for snippet in sorted(list(set(cleaning_snippets))):\n",
        "            print(snippet)\n",
        "    else:\n",
        "        print(\"Keine gr√∂√üeren Probleme erkannt. Daten scheinen sauber zu sein.\")\n",
        "    print('*' * 50)\n",
        "    # =============================\n",
        "\n",
        "    # ====== MODUL 4: ML-RELEVANTE ANALYSE ======\n",
        "    print('*' * 10, 'MODUL 4: ML-RELEVANTE ANALYSE', '*' * 10)\n",
        "    if ml_report_data:\n",
        "        print('*' * 50)\n",
        "        ml_report_df = pd.DataFrame(ml_report_data)\n",
        "        print(\"--- Verteilung und Ausrei√üer ---\")\n",
        "        print(ml_report_df.to_string(index=False))\n",
        "        print('*' * 50)\n",
        "        print(\"\\n--- Hohe Korrelation (>|0.7|) ---\")\n",
        "        if corr_report:\n",
        "            corr_df = pd.DataFrame(corr_report)\n",
        "            print(corr_df.to_string(index=False))\n",
        "        else:\n",
        "            print(\"Keine hohen Korrelationen (>|0.7|) gefunden.\")\n",
        "        print('*' * 50)\n",
        "        print(\"\\n\" + \"Vorschl√§ge zur Vorverarbeitung f√ºr ML:\")\n",
        "        for snippet in sorted(list(set(ml_cleaning_snippets))):\n",
        "            print(snippet)\n",
        "    else:\n",
        "        print(\"Keine numerischen Spalten f√ºr die ML-Analyse gefunden.\")\n",
        "    print(\"\\n\" + \"Der Bericht wurde erfolgreich generiert.\")\n",
        "    print('*' * 50)\n",
        "    # =============================\n",
        "    return cleaning_snippets, ml_cleaning_snippets\n",
        "\n",
        "# HAUPTTEIL DES SKRIPTS\n",
        "if 'df' in locals() and isinstance(df, pd.DataFrame):\n",
        "    print(\"Analysiere Daten und erstelle den Bericht...\")\n",
        "    TEMP_Clear, TEMP_Clear_ML = muster_df(df)\n",
        "    print(\"\\n\" + \"*\" * 50)\n",
        "    print(\"BEREIT ZUR AUTOMATISCHEN BEREINIGUNG!\")\n",
        "    print(\"Sie k√∂nnen jetzt einfach den Befehl 'bereinige(df, TEMP_Clear, TEMP_Clear_ML)' ausf√ºhren.\")\n",
        "else:\n",
        "    print(\"Bitte laden Sie Ihren Datensatz in einen Pandas DataFrame namens 'df'!\")\n",
        "print('*' * 50)\n",
        "\n",
        "print(\"\\n--- Individuell ausl√∂sbare Funktionen ---\")\n",
        "print(\"Folgende Funktionen k√∂nnen einzeln aufgerufen werden, um spezifische Aufgaben auszuf√ºhren:\")\n",
        "print(\"- `analyze_semantic_type_v3(df)`: F√ºhrt eine semantische Analyse durch und gibt einen DataFrame mit den erkannten Typen zur√ºck.\")\n",
        "print(\"- `_correct_spelling_and_expand_abbr(text)`: Korrigiert und erweitert Abk√ºrzungen in einem einzelnen Textstring.\")\n",
        "print(\"- `generate_cleaning_code(column, semantic_type)`: Generiert einen spezifischen Bereinigungscode f√ºr eine Spalte.\")\n",
        "print(\"- `_fill_missing_with_pattern_mode(df, target_col, group_col)`: F√ºllt fehlende Werte basierend auf dem Modus einer Gruppenspalte.\")\n",
        "print(\"- `_find_best_grouping_column(df, target_col, df_analysis)`: Findet die am besten geeignete Gruppierungsspalte f√ºr die Missing-Value-Imputation.\")\n",
        "print(\"- `_find_semantic_groups(df_analysis)`: Identifiziert semantische Spaltengruppen und schl√§gt Feature Engineering vor.\")\n"
      ],
      "metadata": {
        "id": "n1pJ9PM1jag0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38878c9f-4dd4-46ac-d1df-94f9b99437e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bitte laden Sie Ihren Datensatz in einen Pandas DataFrame namens 'df'!\n",
            "**************************************************\n",
            "\n",
            "--- Individuell ausl√∂sbare Funktionen ---\n",
            "Folgende Funktionen k√∂nnen einzeln aufgerufen werden, um spezifische Aufgaben auszuf√ºhren:\n",
            "- `analyze_semantic_type_v3(df)`: F√ºhrt eine semantische Analyse durch und gibt einen DataFrame mit den erkannten Typen zur√ºck.\n",
            "- `_correct_spelling_and_expand_abbr(text)`: Korrigiert und erweitert Abk√ºrzungen in einem einzelnen Textstring.\n",
            "- `generate_cleaning_code(column, semantic_type)`: Generiert einen spezifischen Bereinigungscode f√ºr eine Spalte.\n",
            "- `_fill_missing_with_pattern_mode(df, target_col, group_col)`: F√ºllt fehlende Werte basierend auf dem Modus einer Gruppenspalte.\n",
            "- `_find_best_grouping_column(df, target_col, df_analysis)`: Findet die am besten geeignete Gruppierungsspalte f√ºr die Missing-Value-Imputation.\n",
            "- `_find_semantic_groups(df_analysis)`: Identifiziert semantische Spaltengruppen und schl√§gt Feature Engineering vor.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Explorative Datenanalyse (EDA)\n",
        "\n",
        "Deskriptive Statistik: Berechnung von Mittelwert, Median, Standardabweichung, Korrelationen usw.\n",
        "\n",
        "Datenvisualisierung: Erstellung von Diagrammen wie Histogrammen, Streudiagrammen oder Box-Plots, um die Verteilung der Daten und die Beziehungen zwischen den Variablen zu verstehen."
      ],
      "metadata": {
        "id": "3vnMTjaMnCHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tabulate import tabulate\n",
        "import numpy as np\n",
        "\n",
        "print('df_sem_types_v3 muss vorher als datei tabelle vorhanden sein und CSV mus als df bereinigt und gespeichert sein')\n",
        "\n",
        "# Annahme: Ihre DataFrames 'df' und 'df_sem_types_v3' sind bereits geladen.\n",
        "numerical_cols = df_sem_types_v3[df_sem_types_v3['Semantischer Typ'].isin([\n",
        "    'Float (Waehrung)', 'Float (Prozentsatz)', 'Integer','float64','integer'\n",
        "])]['Spalte'].tolist()\n",
        "\n",
        "categorical_cols = df_sem_types_v3[df_sem_types_v3['Semantischer Typ'].isin([\n",
        "    'Text (Gender)','Text (Kategorisch)', 'Text (object)', 'Text (Freitext)', 'Boolean', 'bool', 'object'\n",
        "])]['Spalte'].tolist()\n",
        "\n",
        "numerical_cols_filtered = [col for col in numerical_cols if col in df.columns]\n",
        "categorical_cols_filtered = [col for col in categorical_cols if col in df.columns]\n",
        "\n",
        "# Erstellen einer Kopie der kategorialen Daten f√ºr die H√§ufigkeitsz√§hlung\n",
        "df_analysis = df[categorical_cols_filtered].copy()\n",
        "\n",
        "# Hinzuf√ºgen der gebinnten numerischen Spalten\n",
        "for col in numerical_cols_filtered:\n",
        "    if df[col].nunique() > 10:\n",
        "        df_analysis[col + '_binned'] = pd.cut(df[col], bins=10, labels=[f'Bin_{i+1}' for i in range(10)])\n",
        "\n",
        "def create_boxplot_and_describe(df, x_col, y_col, hue_col=None):\n",
        "    \"\"\"\n",
        "    Erstellt einen Boxplot mit dynamischer Achsenanpassung und\n",
        "    zeigt die statistischen Kennzahlen f√ºr jede Box.\n",
        "    \"\"\"\n",
        "    if df[y_col].isnull().all() or df[x_col].isnull().all():\n",
        "        print(f\"Keine Daten f√ºr Boxplot von '{y_col}' nach '{x_col}'.\")\n",
        "        return\n",
        "\n",
        "    grouping_cols = [x_col]\n",
        "    if hue_col:\n",
        "        grouping_cols.append(hue_col)\n",
        "\n",
        "    grouped_stats = df.groupby(grouping_cols)[y_col].describe().round(2)\n",
        "    q1_all = grouped_stats['25%']\n",
        "    q3_all = grouped_stats['75%']\n",
        "    iqr_all = q3_all - q1_all\n",
        "\n",
        "    lower_bound = (q1_all - 1.5 * iqr_all).min()\n",
        "    upper_bound = (q3_all + 1.5 * iqr_all).max()\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n",
        "    ax = sns.boxplot(x=x_col, y=y_col, hue=hue_col, data=df, showfliers=False)\n",
        "    ax.set_ylim(lower_bound, upper_bound)\n",
        "\n",
        "    if hue_col:\n",
        "        plt.title(f'Boxplot von {y_col} nach {x_col} und {hue_col}')\n",
        "    else:\n",
        "        plt.title(f'Boxplot von {y_col} nach {x_col}')\n",
        "\n",
        "    sns.despine(left=True, bottom=True)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"Statistische Kennzahlen f√ºr jede Box (Y = '{y_col}', X = '{x_col}', Hue = '{hue_col if hue_col else 'None'}'):\")\n",
        "    print(\"=\"*80)\n",
        "    print(tabulate(grouped_stats, headers='keys', tablefmt='psql'))\n",
        "    print(\"=\"*80)\n",
        "\n",
        "print(\"Analysiere H√§ufigkeiten und erstelle eine Heatmap f√ºr die √úbersicht...\")\n",
        "# Erstellen der Heatmaps basierend auf H√§ufigkeiten\n",
        "for num_col_binned in [c for c in df_analysis.columns if c.endswith('_binned')]:\n",
        "    for cat_col in categorical_cols_filtered:\n",
        "        if df[cat_col].nunique() > 20:\n",
        "            print(f\"*** √úberspringe kategoriale Spalte '{cat_col}': Zu viele eindeutige Werte ({df[cat_col].nunique()} > 20).\")\n",
        "            continue\n",
        "\n",
        "        # Kreuztabelle erstellen, um die H√§ufigkeit zu z√§hlen\n",
        "        contingency_table = pd.crosstab(df_analysis[cat_col], df_analysis[num_col_binned])\n",
        "\n",
        "        # Normieren der Daten, um die relative H√§ufigkeit anzuzeigen\n",
        "        contingency_table_norm = contingency_table.div(contingency_table.sum(axis=1), axis=0)\n",
        "\n",
        "        plt.figure(figsize=(15, 1))\n",
        "        sns.heatmap(contingency_table_norm, annot=True, fmt=\".2f\", cmap='viridis')\n",
        "        plt.title(f'Relative H√§ufigkeit von \"{num_col_binned}\" nach \"{cat_col}\"')\n",
        "        plt.xlabel(num_col_binned)\n",
        "        plt.ylabel(cat_col)\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Statistische Kennzahlen f√ºr die visuell analysierten Spalten erstellen: Y='{num_col_binned.replace('_binned', '')}', X='{cat_col}'\")\n",
        "        create_boxplot_and_describe(df, x_col=cat_col, y_col=num_col_binned.replace('_binned', ''))\n",
        "        print(f\"Boxplot f√ºr die visuell analysierten Spalten erstellen: Y='{num_col_binned.replace('_binned', '')}', X='{cat_col}'\")\n",
        "        print('*' * 50)\n",
        "        print('/' * 50)"
      ],
      "metadata": {
        "id": "ZT_4gIxvjVoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Datenmodellierung und Analyse\n",
        "\n",
        "Statistische Analyse: Verwendung von Hypothesentests, Regressionsanalyse oder Zeitreihenanalyse, um Beziehungen zu validieren.\n",
        "\n",
        "Maschinelles Lernen: Entwicklung von Modellen (z.B. Klassifikation, Regression oder Clustering), um Vorhersagen zu treffen oder Muster zu erkennen"
      ],
      "metadata": {
        "id": "5gfQ_LlYnaKx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Interpretation und Kommunikation der Ergebnisse\n",
        "\n",
        "- Ergebnisinterpretation: Was bedeuten die Ergebnisse im Kontext der urspr√ºnglichen Problemstellung?\n",
        "\n",
        "- Visualisierung der Ergebnisse: Erstellung von klaren und √ºberzeugenden Dashboards, Berichten oder Pr√§sentationen, um die Erkenntnisse zu veranschaulichen.\n",
        "\n",
        "- Storytelling: Die Ergebnisse in eine narrative Form bringen, die es den Stakeholdern erm√∂glicht, die Schlussfolgerungen leicht zu verstehen und darauf basierend Entscheidungen zu treffen."
      ],
      "metadata": {
        "id": "-1z9doL_nkf4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KI anforderungen\n"
      ],
      "metadata": {
        "id": "cMBkxCmt22dr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anweisungen f√ºr die Python-Code-Erstellung\n",
        "Der generierte Python-Code muss sich strikt an die folgenden Regeln halten. Es wird angenommen, dass ein DataFrame namens df bereits existiert und geladen ist. Es d√ºrfen keine Code-Abschnitte eingef√ºgt werden, die einen DataFrame erstellen oder zuf√§llige Daten einbinden.\n",
        "\n",
        "1. Code-Formatierung und Struktur\n",
        "PEP 8 Konformit√§t: Halte dich an die Richtlinien von PEP 8, inklusive vier Leerzeichen Einr√ºckung, snake_case f√ºr Funktionen/Variablen und CamelCase f√ºr Klassennamen.\n",
        "\n",
        "Docstrings: Verwende f√ºr jede Funktion und Klasse einen Google-Style Docstring, der den Zweck, die Parameter (Args) und den R√ºckgabewert (Returns) beschreibt.\n",
        "\n",
        "2. Standard-Struktur der Code-Abschnitte\n",
        "Jeder Code-Abschnitt muss eindeutig mit klaren Markierungen beginnen und enden, sowohl im Code als auch in der Ausgabe.\n",
        "\n",
        "Anfang:\n",
        "\n",
        "'# ====== [√úBERTHEMA] ======  (mit dem jeweiligen Thema)\n",
        "\n",
        "print('*' * 10, '[√úBERTHEMA]', '*' * 10)\n",
        "\n",
        "Ende:\n",
        "\n",
        "print('*' * 50)\n",
        "\n",
        "'# =============================\n",
        "\n",
        "Typ-Anmerkungen: Nutze konsequent Typ-Anmerkungen f√ºr alle Funktionsargumente, R√ºckgabewerte und Variablen.\n",
        "\n",
        "Inhaltsbasierte Logik: Die Klassifizierung muss st√§rker auf dem Inhalt der Spalte basieren, nicht nur auf dem Spaltennamen.\n",
        "\n",
        "Best√§tigungspr√ºfung: Jede Klassifizierung muss durch eine zus√§tzliche logische Pr√ºfung untermauert werden. Eine Spalte, die als 'Datum/Zeit' klassifiziert wird, muss auch tats√§chlich in das datetime-Format konvertierbar sein.\n",
        "\n",
        "Verbesserung der Trefferquote:z.b. F√ºr jeden semantischen Typ muss eine Kombination aus Namens- und Inhaltsmerkmalen herangezogen werden, um die Wahrscheinlichkeit eines korrekten Treffers zu erh√∂hen. dieses verstendnis muss im programm inteligenz eingebunden werden.\n",
        "\n",
        "3. Sichere Datenverarbeitung & Spaltenerstellung\n",
        "Wende die folgende Logik f√ºr jeden Abschnitt an, der df bearbeitet oder erweitert.\n",
        "\n",
        "Tempor√§re Kopie: Erstelle eine tempor√§re Kopie f√ºr den Testlauf.\n",
        "\n",
        "Wenn len(df) > 1000, nutze nur die ersten 1000 Zeilen: TEMP_df = df.head(1000).copy().\n",
        "\n",
        "Wenn len(df) <= 1000, nutze den gesamten DataFrame: TEMP_df = df.copy().\n",
        "\n",
        "Testlauf und √úberpr√ºfung: F√ºhre die geplante Operation (Datenbereinigung, Transformation, neue Spalten generieren etc.) nur auf TEMP_df aus. Die Funktion muss einen boolean Wert f√ºr Erfolg oder Misserfolg zur√ºckgeben.\n",
        "\n",
        "Anwendung auf Originaldaten: Wende die Funktion auf den Original-DataFrame (df) nur an, wenn der Testlauf auf TEMP_df erfolgreich war. Im Falle eines Fehlers darf df nicht ver√§ndert werden.\n",
        "\n",
        "4. Ausgabe als Erfolgsnachweis\n",
        "Nach einem erfolgreichen und manipulierenden Datenverarbeitungs-Abschnitt m√ºssen folgende Informationen ausgegeben werden.\n",
        "\n",
        "Dynamischer Nachweis des Erfolgs: Gib eine klare und pr√§gnante Ausgabe, die den Erfolg der Datenmanipulation eindeutig beweist. Der Nachweis muss zur Art der vorgenommenen √Ñnderung passen.\n",
        "\n",
        "Zum Beispiel:\n",
        "\n",
        "Wenn Zeilen gel√∂scht wurden, zeige die Zeilenzahl vorher und nachher.\n",
        "\n",
        "Wenn NaN-Werte entfernt wurden, zeige die Anzahl der fehlenden Werte vorher und nachher.\n",
        "\n",
        "Wenn neue Spalten erstellt wurden, zeige die Namen der neuen Spalten.\n",
        "\n",
        "Datenprobe: Gib die ersten 5 Zeilen des endg√ºltigen, bearbeiteten DataFrames als Tabelle aus, einschlie√ülich der Spalten√ºberschriften: print(df_final.head(5).to_string()).\n",
        "\n",
        "5. Spezifische Regeln f√ºr Machine Learning\n",
        "Standardverhalten: Wenn der Zweck des Codes rein analytisch ist (z.B. Modell-Evaluierung, Datenexploration), darf der Original-DataFrame df nicht ver√§ndert werden. Die Verarbeitung findet nur auf der tempor√§ren Kopie statt.\n",
        "\n",
        "Manuelle Ausl√∂sung: Wenn die Bearbeitung des df explizit verlangt wird, muss der Code den Nutzer nach der Anzahl der Iterationen fragen.\n",
        "\n",
        "try: anzahl_durchlaeufe = int(input(\"Wie oft soll der Prozess wiederholt werden? (Geben Sie eine Zahl ein): \")) print(f\"Der Prozess wird {anzahl_durchlaeufe} Mal ausgef√ºhrt.\") except ValueError: print(\"Ung√ºltige Eingabe. Der Prozess wird einmalig ausgef√ºhrt.\")\n",
        "\n",
        "Ausgabe des tempor√§ren DataFrames: Am Ende des Ablaufs soll der Code eine Anweisung ausgeben, wie der Prozess erneut mit dem tempor√§ren Datensatz ausgel√∂st werden kann:\n",
        "\n",
        "print(f\"\\nUm den gleichen Ablauf zu wiederholen, verwenden Sie '{TEMP_df_name}' anstelle von 'df'.\")\n",
        "\n",
        "---\n",
        "\n",
        "Zusammenfassung:\n",
        "Eine kurze, pr√§gnante Beschreibung des Erfolgs der durchgef√ºhrten Datenverarbeitung.\n",
        "\n",
        "Beispiel f√ºr Bereinigung: \"Es wurden {Anzahl} Spalten analysiert und f√ºr eine Bereinigung vorgeschlagen.\"\n",
        "\n",
        "Beispiel f√ºr Manipulation: \"Es wurden {Anzahl_vorher} Zeilen bereinigt und {Anzahl_nachher} Zeilen verbleiben.\"\n",
        "\n",
        "Beispiel Datenprobe: Die ersten 5 Zeilen des endg√ºltigen DataFrames, um die erfolgreiche Verarbeitung visuell zu best√§tigen.\n",
        "\n",
        "---\n",
        "\n",
        "Aktionsplan f√ºr die Bereinigung: Eine separate Liste von Code-Snippets, die direkt kopiert und ausgef√ºhrt werden k√∂nnen, um die vorgeschlagenen Bereinigungsschritte umzusetzen. Jeder Code-Abschnitt steht auf einer neuen Zeile, um das Kopieren zu erleichtern."
      ],
      "metadata": {
        "id": "u9o2Qmnz0EPH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Damit wird der gesamte modulare Plan wie folgt aussehen:\n",
        "\n",
        "Modul 1: Allgemeine √úbersicht (kompakte Tabelle)\n",
        "\n",
        "Modul 2: Statistische Kennzahlen in logischer Verteilung (wie oben beschrieben)\n",
        "\n",
        "Modul 3: Erkannte Probleme & Bereinigungsvorschl√§ge (Liste)\n",
        "\n",
        "Sind Sie mit diesem vollst√§ndigen Plan einverstanden, damit ich den finalen Code erstellen kann?"
      ],
      "metadata": {
        "id": "RZ6hSnSaw5NX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Verbesserungspunkte (alles \"Insufficient\")\n",
        "\n",
        "Data Validation\n",
        "\n",
        "Du hast nicht alle Variablen gepr√ºft oder Datenbereinigung durchgef√ºhrt.\n",
        "\n",
        "Hier fehlt also eine klare Dokumentation von:\n",
        "\n",
        "Umgang mit fehlenden Werten\n",
        "\n",
        "Dubletten, Ausrei√üer\n",
        "\n",
        "Datentyp-√úberpr√ºfung (Strings vs. Zahlen etc.)\n",
        "\n",
        "ggf. Normalisierung/Standardisierung\n",
        "\n",
        "Business Metrics\n",
        "\n",
        "Dir fehlen definierte Kennzahlen, die das Unternehmen sp√§ter zur Erfolgsmessung nutzen kann.\n",
        "\n",
        "Beispiele:\n",
        "\n",
        "Conversion Rate, Churn Rate, Net Promoter Score, Umsatzwachstum\n",
        "\n",
        "Vorher-Nachher-Vergleich mit Daten (Baseline setzen!)\n",
        "\n",
        "- Mein Tipp f√ºr die n√§chste Pr√ºfung\n",
        "\n",
        "Bei Data Validation:\n",
        "Mach einen klaren Abschnitt in deiner Analyse, wo du explizit beschreibst, welche Checks du gemacht hast und welche Daten du bereinigt hast. Am besten mit Code (z. B. in Python/Pandas: .info(), .isnull().sum(), .duplicated().sum()).\n",
        "\n",
        "Bei Business Metrics:\n",
        "Formuliere 1‚Äì2 klare KPIs, die das Unternehmen langfristig √ºberwachen kann. Erstelle mit den vorhandenen Daten einen Ausgangswert (Baseline), damit man den Erfolg sp√§ter messen kann."
      ],
      "metadata": {
        "id": "tMOqXq-l8tma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Validation (=> Insufficient)\n",
        "\n",
        "Was erwartet wird\n",
        "\n",
        "Vollst√§ndige √úbersicht aller Variablen (Name, Datentyp, fehlende Werte, Anzahl eindeutiger Werte).\n",
        "\n",
        "Dokumentierte Reinigungs-Schritte (Duplikate, fehlende Werte: welche Strategie, Outlier-Behandlung, Typkonversionen).\n",
        "\n",
        "Begr√ºndung jeder √Ñnderung (warum fehlende Werte imputiert/gel√∂scht wurden).\n",
        "\n",
        "Vorher-/Nachher-Statistiken (z. B. Zeilenanzahl, % fehlender Werte).\n",
        "\n",
        "Belege, die du zeigen solltest\n",
        "\n",
        "Tabelle/Output: df.info(), df.describe() und df.isnull().sum().\n",
        "\n",
        "Beispielhafte Code-Snippets + Ergebnis (z. B. Anzahl gel√∂schter Duplikate).\n",
        "\n",
        "Ein kurzes ‚ÄûData cleaning log‚Äú: Liste der Aktionen + Effekt (z. B. ‚ÄûRemoved 12 duplicates; imputed 5% missing using median for column X‚Äú).\n",
        "\n",
        "Konkrete Verbesserungen (To-Do)\n",
        "\n",
        "F√ºge eine Variable-√úbersichtstabelle ein.\n",
        "\n",
        "Dokumentiere Handling jeder Spalte (z. B. ‚ÄûSpalte: price ‚Äî Datentyp float, 2% missing ‚Äî Strategy: median imputation, Reason: skewed distribution‚Äú).\n",
        "\n",
        "Zeige einfache Outlier-Checks (IQR oder z-score) und was du damit gemacht hast."
      ],
      "metadata": {
        "id": "_aX8O3DK9bOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# √úberblick\n",
        "df.info()\n",
        "df.isnull().sum().sort_values(ascending=False)\n",
        "\n",
        "# Duplikate & Beispiele\n",
        "print(\"duplicates:\", df.duplicated().sum())\n",
        "df[df.duplicated()].head()\n",
        "\n",
        "# einfache Outlier-Check (IQR)\n",
        "Q1 = df['numeric_col'].quantile(0.25)\n",
        "Q3 = df['numeric_col'].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "outliers = df[(df['numeric_col'] < Q1 - 1.5*IQR) | (df['numeric_col'] > Q3 + 1.5*IQR)]\n",
        "len(outliers)"
      ],
      "metadata": {
        "id": "KBV4IuC49ggF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Visualization (=> Sufficient)\n",
        "\n",
        "- Was erwartet wird\n",
        "\n",
        "Mind. 2 verschiedene Visualisierungen einzelner Variablen (z. B. Histogramm, Boxplot, Balken).\n",
        "\n",
        "Mind. 1 Visualisierung mit mehreren Variablen (z. B. Scatterplot, Grouped bar, Heatmap).\n",
        "\n",
        "Jede Visualisierung muss eine kurze Interpretation/Takeaway haben (was sieht man? welche Schlussfolgerung?).\n",
        "\n",
        "Lesbarkeit: Achsenbeschriftungen, Legende, Titel, ggf. Annotationen.\n",
        "\n",
        "- Belege, die du zeigen solltest\n",
        "\n",
        "F√ºr jede Grafik: Bild + 1 Satz (Takeaway) + welche Daten-Transformation ggf. vorher gemacht wurde.\n",
        "\n",
        "Wenn du mehrere Charts vergleichst: zeige direkte Vergleichswerte (z. B. medians, correlation coefficients).\n",
        "\n",
        "- Feinschliff / Quick wins\n",
        "\n",
        "Schreibe unter jede Abbildung 1‚Äì2 bullet points: ‚ÄúWas zeigt die Grafik?‚Äù und ‚ÄúWarum ist es relevant f√ºr das Business?‚Äù.\n",
        "\n",
        "Bei Scatterplots: f√ºge Korrelationswert (r) hinzu und ggf. eine Regressionslinie."
      ],
      "metadata": {
        "id": "E0Y9ibWM9i1-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Business Focus (=> Sufficient)\n",
        "\n",
        "- Was erwartet wird\n",
        "\n",
        "Mind. ein klares Business-Ziel (z. B. ‚ÄûReduktion Churn um X%‚Äú, ‚ÄûUmsatz pro Kunde erh√∂hen‚Äú).\n",
        "\n",
        "Klare Verbindung: Wie tr√§gt deine Analyse dazu bei, das Ziel zu l√∂sen? (z. B. Segmentierung ‚Üí Zielgerichtete Kampagnen).\n",
        "\n",
        "Mind. eine konkrete Empfehlung f√ºr weitere Schritte.\n",
        "\n",
        "- Belege, die du zeigen solltest\n",
        "\n",
        "Kurze Formulierung des Ziels + wie deine Ergebnisse Einfluss nehmen (z. B. ‚ÄûSegment A zeigt 30% h√∂here Conversion ‚Üí Fokus auf A lohnt sich‚Äú).\n",
        "\n",
        "Priorisierte Empfehlung (z. B. quick win vs. langfristige Ma√ünahme).\n",
        "\n",
        "- Verbesserungstipp\n",
        "\n",
        "Erg√§nze Verantwortliche + Zeithorizont f√ºr jede Empfehlung (z. B. ‚ÄûMarketing ‚Äî 3 Monate ‚Äî A/B Test‚Äú)"
      ],
      "metadata": {
        "id": "TnEmyZiA9puR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Business Metrics (=> Insufficient)\n",
        "\n",
        "- Was erwartet wird\n",
        "\n",
        "Definition von mindestens einem messbaren KPI, den das Business zur Erfolgsmessung nutzen kann.\n",
        "\n",
        "Berechnung eines Baseline-Werts aus den vorhandenen Daten (aktueller Stand).\n",
        "\n",
        "Monitoring-Plan: Frequenz (daily/weekly/monthly), Schwellen/Targets, Verantwortlicher.\n",
        "\n",
        "- Konkrete Beispiele f√ºr KPIs\n",
        "\n",
        "Conversion Rate = K√§ufe / Sessions\n",
        "\n",
        "Churn Rate = abgewanderte Kunden / Kundenbestand\n",
        "\n",
        "Average Order Value (AOV) = Umsatz / Anzahl Bestellungen\n",
        "\n",
        "Retention nach 30 Tagen (%)\n",
        "\n",
        "- Was du zeigen musst\n",
        "\n",
        "KPI-Formel + kurzer Satz, warum KPI relevant ist.\n",
        "\n",
        "Baseline-Berechnung (Tabellen/Plot).\n",
        "\n",
        "Zielsetzung (z. B. ‚ÄûZiel: Conversion +2% in 3 Monaten‚Äú) und wie man das misst."
      ],
      "metadata": {
        "id": "EY2TyIT499ho"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Beispiel: Conversion Rate baseline\n",
        "daily = df.groupby('date').agg({'sessions':'sum','purchases':'sum'}).reset_index()\n",
        "daily['conv_rate'] = daily['purchases'] / daily['sessions']\n",
        "baseline = daily['conv_rate'].mean()\n",
        "print(\"Baseline conversion rate:\", baseline)\n",
        "\n",
        "# Zeige Trend\n",
        "daily[['date','conv_rate']].plot(x='date', y='conv_rate')"
      ],
      "metadata": {
        "id": "upRLvXi1-NlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To-Do\n",
        "\n",
        "Definiere 1‚Äì2 KPIs, reiche Formel + baseline + Chart ein.\n",
        "\n",
        "Erg√§nze: Wer √ºberwacht KPI, wie oft und welches Ziel (z. B. +5% YOY)."
      ],
      "metadata": {
        "id": "xhzdoLlc-RxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Communication (=> Sufficient)\n",
        "\n",
        "- Was erwartet wird\n",
        "\n",
        "F√ºr jeden Analyse-Schritt: kurze schriftliche Erkl√§rung deiner Erkenntnisse / Methoden.\n",
        "\n",
        "Pr√§sentation: enth√§lt Business-Ziele, Hauptergebnisse, Empfehlungen (executive summary).\n",
        "\n",
        "Erz√§hlstruktur: Problem ‚Üí Vorgehen ‚Üí Erkenntnisse ‚Üí Handlungsempfehlungen.\n",
        "\n",
        "- Feinschliff\n",
        "\n",
        "Executive-Summary Slide (1 Folie): 3 Kernaussagen + 1 KPI-Tabelle (Baseline + empfohlene Zielwerte).\n",
        "\n",
        "Bei Folien: 1 Insight pro Folie, gro√üe Schrift, klare Visuals."
      ],
      "metadata": {
        "id": "kRHQiLtN-V0D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Priorisierte, konkrete n√§chste Schritte (3-Stufen Plan)\n",
        "\n",
        "1 Data Validation (urgent) ‚Äî Erstelle ein kurzes Notebook/Abschnitt: Variablen-√úbersicht, missing/dupes, 3 Beispiele f√ºr Cleaning + Vorher/Nachher-Zahlen.\n",
        "\n",
        "2 Business Metrics ‚Äî W√§hle 1 KPI, berechne Baseline aus deinen Daten, zeige Trendplot und setze ein realistisches Ziel.\n",
        "\n",
        "3 Presentation polish ‚Äî Executive Summary + unter jeder Grafik 1 Takeaway-Satz Handlungsempfehlung."
      ],
      "metadata": {
        "id": "UC_2yVgt-fmd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "aVeh8PLt-nol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Muster CODE zu Daten Verwaltung"
      ],
      "metadata": {
        "id": "RWy22hb2iSs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laden der daten"
      ],
      "metadata": {
        "id": "qERBsNK9c25f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ‚öôÔ∏è Laden von CSV-Dateien in Colab\n",
        "# ============================================================\n",
        "\n",
        "print('*' * 10, 'DATEI-UPLOAD', '*' * 10)\n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "print(\"\\nDie Datei wurde erfolgreich hochgeladen und steht nun zur Verf√ºgung.\")\n",
        "\n",
        "print('*' * 50)\n",
        "# ============================="
      ],
      "metadata": {
        "id": "wu-p7jgMc1kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ‚öôÔ∏è DataFrames als CSV-Datei in df benennen\n",
        "# ============================================================\n",
        "\n",
        "print('*' * 10, 'DATEN LADEN', '*' * 10)\n",
        "\n",
        "# Angenommen, die Datei 'vehicle.csv' wurde bereits hochgeladen\n",
        "df = pd.read_csv('vehicle.csv')\n",
        "\n",
        "# Erfolgsnachweis: Ausgabe der Dateninformationen\n",
        "print(f\"Daten erfolgreich geladen! DataFrame-Gr√∂√üe: {df.shape}\")\n",
        "print(\"\\nErste 5 Zeilen des geladenen DataFrames zur √úberpr√ºfung:\")\n",
        "print(df.head().to_string())\n",
        "\n",
        "print('*' * 50)\n",
        "# ============================="
      ],
      "metadata": {
        "id": "71nFLMsBc8iw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exportieren der daten und erstelung der TEMP\n"
      ],
      "metadata": {
        "id": "ywv9HtidcX2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ‚öôÔ∏è Exportieren des DataFrames als CSV-Datei\n",
        "# ============================================================\n",
        "\n",
        "import time\n",
        "from google.colab import files\n",
        "\n",
        "# Automatische Kopie von df erstellen\n",
        "TEMP = df.copy()\n",
        "\n",
        "# Eingabe vom User\n",
        "filename = input(\"Bitte Dateiname f√ºr Export eingeben (Enter f√ºr tempor√§ren Namen): \").strip()\n",
        "\n",
        "# Wenn kein Name angegeben wird -> tempor√§ren Namen erstellen\n",
        "if not filename:\n",
        "    timestamp = int(time.time())  # aktueller UNIX-Timestamp\n",
        "    filename = f\"temp_export_{timestamp}.csv\"\n",
        "\n",
        "# Falls der User keinen .csv angeh√§ngt hat, automatisch erg√§nzen\n",
        "if not filename.endswith(\".csv\"):\n",
        "    filename += \".csv\"\n",
        "\n",
        "# Speichern CSV im Colab-Dateisystem\n",
        "# index=False, um den Pandas-Index nicht in die CSV zu schreiben\n",
        "TEMP.to_csv(filename, index=False)\n",
        "print(f\"\\n'{filename}' wurde im Colab-Dateisystem erstellt.\")\n",
        "\n",
        "# Download\n",
        "files.download(filename)"
      ],
      "metadata": {
        "id": "WiaaA0dYcdQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laden der daten mit TEMP lade funktion\n"
      ],
      "metadata": {
        "id": "PxPO7TGrcc4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ‚öôÔ∏è Laden von CSV-Dateien in Colab\n",
        "# ============================================================\n",
        "\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# Nutzer-Eingabe\n",
        "choice = input(\"M√∂chten Sie eine Datei hochladen? (ja = hochladen, Enter = TEMP laden): \").strip().lower()\n",
        "\n",
        "if choice == \"ja\":\n",
        "    # User m√∂chte selbst eine Datei ausw√§hlen\n",
        "    uploaded = files.upload()\n",
        "    filename = list(uploaded.keys())[0]\n",
        "    df = pd.read_csv(filename)\n",
        "    print(f\"\\n'{filename}' wurde erfolgreich geladen.\")\n",
        "else:\n",
        "    # Statt neue Datei laden -> vorhandene TEMP nutzen\n",
        "    try:\n",
        "        df = TEMP\n",
        "        print(\"\\nVorhandene TEMP-Kopie wurde geladen.\")\n",
        "    except NameError:\n",
        "        raise ValueError(\"Keine TEMP-Kopie gefunden! Bitte zuerst eine Datei exportieren oder hochladen.\")"
      ],
      "metadata": {
        "id": "WAP7hJi1cnbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Geschefts statistick"
      ],
      "metadata": {
        "id": "F4-aZ1IGIm4P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## √úbersicht der daten df mit KPI\n"
      ],
      "metadata": {
        "id": "UkvULsLudk3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# üìä DATA VALIDATION & BUSINESS METRICS PIPELINE\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "# ============================================================\n",
        "# 1. Basic Overview\n",
        "# ============================================================\n",
        "print(\"************* Dataset Overview *************\")\n",
        "num_rows, num_cols = df.shape\n",
        "print(f\"The DataFrame has {num_rows:,} rows and {num_cols} columns.\")\n",
        "print(df.info())\n",
        "print(\"/\" * 40)\n",
        "\n",
        "# ============================================================\n",
        "# 2. Missing Values Check\n",
        "# ============================================================\n",
        "print(\"************* Missing Values *************\")\n",
        "null_summary = pd.DataFrame({\n",
        "    'Null Count': df.isnull().sum(),\n",
        "    'Null %': (df.isnull().sum() / len(df) * 100).round(2)\n",
        "})\n",
        "print(null_summary)\n",
        "print(\"‚Üí Business Impact: Revenue has missing values (7%). \"\n",
        "      \"Important for sales analysis ‚Üí must impute or flag.\")\n",
        "print(\"/\" * 40)\n",
        "\n",
        "# ============================================================\n",
        "# 3. Special Characters / Data Consistency\n",
        "# ============================================================\n",
        "print(\"************* Special Character Check *************\")\n",
        "def count_special_chars(text):\n",
        "    if isinstance(text, str):\n",
        "        return len(re.findall(r'[^a-zA-Z0-9 ]', text))\n",
        "    return 0\n",
        "\n",
        "special_char_counts = df.applymap(count_special_chars).sum()\n",
        "print(special_char_counts)\n",
        "print(\"‚Üí Example issue: 'Email' vs 'email' vs 'em + call'. \"\n",
        "      \"‚Üí Standardization required.\")\n",
        "print(\"/\" * 40)\n",
        "\n",
        "# ============================================================\n",
        "# 4. Descriptive Statistics\n",
        "# ============================================================\n",
        "print(\"************* Numerical Summary *************\")\n",
        "print(df.describe().round(2))\n",
        "print(\"/\" * 40)\n",
        "\n",
        "# ============================================================\n",
        "# 5. Outlier Detection (Boxplot & IQR Method)\n",
        "# ============================================================\n",
        "numeric_cols = ['nb_sold', 'revenue', 'years_as_customer', 'nb_site_visits']\n",
        "\n",
        "for col in numeric_cols:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    outliers = ((df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))).sum()\n",
        "    print(f\"{col}: {outliers} outliers detected\")\n",
        "\n",
        "    # Optional: visualize\n",
        "    plt.figure(figsize=(6,3))\n",
        "    sns.boxplot(x=df[col])\n",
        "    plt.title(f\"Outlier Check: {col}\")\n",
        "    plt.show()\n",
        "\n",
        "print(\"/\" * 40)\n",
        "\n",
        "# ============================================================\n",
        "# 6. Uniqueness & Duplicates\n",
        "# ============================================================\n",
        "print(\"************* Duplicate Check *************\")\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Total duplicate rows: {duplicates}\")\n",
        "print(\"/\" * 40)\n",
        "\n",
        "# ============================================================\n",
        "# 7. Business Logic Check\n",
        "# ============================================================\n",
        "print(\"************* Business Logic Checks *************\")\n",
        "df['unit_price'] = df['revenue'] / df['nb_sold']\n",
        "print(df['unit_price'].describe().round(2))\n",
        "print(\"‚Üí Check: Average unit price should be consistent (not negative, not extreme).\")\n",
        "print(\"/\" * 40)\n",
        "\n",
        "# ============================================================\n",
        "# 8. Business Metrics (KPIs)\n",
        "# ============================================================\n",
        "print(\"************* BUSINESS KPIs *************\")\n",
        "\n",
        "# Revenue per Customer\n",
        "rev_per_customer = df.groupby(\"customer_id\")[\"revenue\"].sum().mean()\n",
        "print(f\"Avg. Revenue per Customer: {rev_per_customer:.2f}\")\n",
        "\n",
        "# Conversion Proxy (site visits -> sales)\n",
        "df['conversion_rate'] = df['nb_sold'] / df['nb_site_visits']\n",
        "avg_conversion = df['conversion_rate'].mean()\n",
        "print(f\"Avg. Conversion Rate: {avg_conversion:.2%}\")\n",
        "\n",
        "# Customer Retention (years_as_customer distribution)\n",
        "retention_rate = (df['years_as_customer'] > 1).mean()\n",
        "print(f\"Retention Rate (>1 year): {retention_rate:.2%}\")\n",
        "\n",
        "print(\"/\" * 40)\n",
        "\n",
        "# ============================================================\n",
        "# 9. Visualization of Business KPIs\n",
        "# ============================================================\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.lineplot(data=df, x=\"week\", y=\"revenue\", estimator=\"mean\", ci=None)\n",
        "plt.title(\"Average Weekly Revenue Trend\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(x=\"sales_method\", y=\"revenue\", data=df)\n",
        "plt.title(\"Revenue by Sales Method\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 10. Executive Summary (printed)\n",
        "# ============================================================\n",
        "print(\"************* EXECUTIVE SUMMARY *************\")\n",
        "print(\"‚úÖ Data Quality:\")\n",
        "print(\"- Missing values: 7% in revenue (imputation required)\")\n",
        "print(\"- Sales method labels inconsistent ‚Üí standardize\")\n",
        "print(\"- Outliers found in revenue and years_as_customer ‚Üí investigate\")\n",
        "\n",
        "print(\"\\n‚úÖ Business Insights:\")\n",
        "print(f\"- Avg. Revenue/Customer = {rev_per_customer:.2f}\")\n",
        "print(f\"- Conversion Rate = {avg_conversion:.2%}\")\n",
        "print(f\"- Retention Rate (>1yr customers) = {retention_rate:.2%}\")\n",
        "\n",
        "print(\"\\n‚úÖ Next Steps:\")\n",
        "print(\"- Clean sales_method categories (Email/Call)\")\n",
        "print(\"- Impute revenue missing values (median or regression)\")\n",
        "print(\"- Monitor Conversion Rate weekly ‚Üí baseline established\")\n"
      ],
      "metadata": {
        "id": "hvvBqge_d_EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Samantic Erkenung"
      ],
      "metadata": {
        "id": "RbH_f7jcfft9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== SEMANTISCHE MUSTER-ERKENNUNG ======\n",
        "print('=' * 10, 'SEMANTISCHE MUSTER-ERKENNUNG', '=' * 10)\n",
        "\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from typing import Dict, Any, List, Callable, Union\n",
        "\n",
        "def generate_cleaning_muster(column: str, semantic_type: str) -> str:\n",
        "    \"\"\"\n",
        "    Generiert einen Python-Muster-Vorschlag zur Datenbereinigung basierend auf dem\n",
        "    erkannten semantischen Typ.\n",
        "\n",
        "    Args:\n",
        "        column (str): Der Name der Spalte.\n",
        "        semantic_type (str): Der erkannte semantische Typ.\n",
        "\n",
        "    Returns:\n",
        "        str: Ein String, der den vorgeschlagenen Bereinigungsmuster enth√§lt.\n",
        "    \"\"\"\n",
        "    if semantic_type == 'Datum/Zeit':\n",
        "        return f\"df['{column}'] = pd.to_datetime(df['{column}'], errors='coerce')\"\n",
        "    elif semantic_type == 'ID':\n",
        "        return f\"df['{column}'] = df['{column}'].astype('object')\"\n",
        "    elif semantic_type == 'Boolean':\n",
        "        return f\"df['{column}'] = df['{column}'].astype(bool)\"\n",
        "    elif semantic_type == 'Integer':\n",
        "        return f\"df['{column}'] = pd.to_numeric(df['{column}'], errors='coerce').astype('Int64')\"\n",
        "    elif semantic_type == 'Float (Geografisch)' or semantic_type == 'Float (Waehrung)' or semantic_type == 'Float (Prozentsatz)':\n",
        "        return f\"df['{column}'] = pd.to_numeric(df['{column}'].str.replace(',', '.'), errors='coerce')\"\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def analyze_semantic_type_v3(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Analysiert die semantischen Datentypen der Spalten in einem DataFrame mit\n",
        "    einer angepassten Logik, bei der Spaltenname und Inhalt f√ºr die\n",
        "    semantische Klassifizierung √ºbereinstimmen m√ºssen.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Der zu analysierende DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Ein DataFrame, der die Spalte, den urspr√ºnglichen\n",
        "        Datentyp, den erkannten semantischen Typ und einen\n",
        "        Bereinigungsvorschlag enth√§lt.\n",
        "    \"\"\"\n",
        "    SEMANTIC_HINTS_PRIORITY: Dict[str, Dict[str, Union[set, Callable]]] = {\n",
        "        'ID': {\n",
        "            'keywords': {'id', 'session_id', 'trip_id', 'user_id', 'unique_id', 'kundennummer', 'bestellnr', 'order_id', 'artikelnummer'},\n",
        "            'validation_func': lambda s: ((s.dropna().astype(str).apply(len) >= 5).any())\n",
        "        },\n",
        "        'Datum/Zeit': {\n",
        "            'keywords': {'week','datum', 'zeit', 'date', 'time', 'start', 'end', 'birthdate', 'signup_date', 'check_in', 'check_out', 'departure', 'return', 'geburtstag', 'timestamp', 'creation_date', 'modified_date', 'erstellt'},\n",
        "            'validation_func': lambda s: (pd.to_datetime(s.dropna(), errors='coerce').notna().all() or (s.dropna().astype(str).str.contains(r'[-_/]', na=False).any() and s.dropna().astype(str).str.contains(r'\\d{4}', na=False).any()))\n",
        "        },\n",
        "        'Geometrisch': {\n",
        "            'keywords': {'geom', 'geometry', 'shape', 'wkt', 'geojson', 'coordinates', 'location_data'},\n",
        "            'validation_func': lambda s: (s.dropna().astype(str).str.contains(r'^(POINT|LINESTRING|POLYGON|MULTIPOINT|MULTILINESTRING|MULTIPOLYGON)\\s*\\(', regex=True, na=False).any() or s.dropna().astype(str).str.contains(r'{\"type\":\\s*\"(Point|LineString|Polygon|MultiPoint|MultiLineString|MultiPolygon)\"', regex=True, na=False).any())\n",
        "        },\n",
        "    }\n",
        "    SEMANTIC_HINTS_TEXT: Dict[str, Dict[str, Union[set, Callable]]] = {\n",
        "        'Text (Kategorisch)': {\n",
        "            'keywords': {'city', 'country', 'l√§nder', 'region', 'state', 'bundesland', 'zip', 'plz'},\n",
        "            'validation_func': lambda s: s.dropna().nunique() >= 2 and (pd.api.types.is_string_dtype(s.dropna()) or isinstance(s.dropna().dtype, pd.CategoricalDtype))\n",
        "        },\n",
        "        'Text (Gender)': {\n",
        "            'keywords': {'geschlecht', 'typ', 'category', 'art', 'gender','method'},\n",
        "            'validation_func': lambda s: s.dropna().nunique() >= 2 and (pd.api.types.is_string_dtype(s.dropna()) or isinstance(s.dropna().dtype, pd.CategoricalDtype))\n",
        "        },\n",
        "        'Text (object)': {\n",
        "            'keywords': {'airport', 'destination', 'origin', 'heimat', 'status'},\n",
        "            'validation_func': lambda s: s.dropna().nunique() >= 2 and (pd.api.types.is_string_dtype(s.dropna()) or isinstance(s.dropna().dtype, pd.CategoricalDtype))\n",
        "        },\n",
        "        'Text (Freitext)': {\n",
        "            'keywords': {'name', 'hotel', 'airline', 'beschreibung', 'kommentar', 'nachricht', 'adresse'},\n",
        "            'validation_func': lambda s: pd.api.types.is_string_dtype(s.dropna()) or isinstance(s.dropna().dtype, pd.CategoricalDtype)\n",
        "        },\n",
        "    }\n",
        "    SEMANTIC_HINTS_NUMERIC: Dict[str, Dict[str, Union[set, Callable]]] = {\n",
        "        'Boolean': {\n",
        "            'keywords': {'boolean', 'bool', 'booked', 'married', 'cancellation', 'children','discount'},\n",
        "            'validation_func': lambda s: (s.dropna().nunique() == 2) and (pd.api.types.is_bool_dtype(s.dropna()) or set(s.dropna().astype(str).str.lower().str.strip().unique()).issubset({'true', 'false', '1', '0', 'ja', 'nein', 'yes', 'no', 't', 'f', 'wahr', 'falsch'}))\n",
        "        },\n",
        "        'Float (Geografisch)': {\n",
        "            'keywords': {'lat', 'lon', 'latitude', 'longitude'},\n",
        "            'validation_func': lambda s: pd.to_numeric(s.dropna(), errors='coerce').notna().all() and (pd.to_numeric(s.dropna(), errors='coerce').astype(str).str.count(r'\\.').all() or pd.api.types.is_float_dtype(s.dropna()))\n",
        "        },\n",
        "        'Float (Prozentsatz)': {\n",
        "            'keywords': {'percent', 'pct', 'rate', 'discount', '%'},\n",
        "            'validation_func': lambda s: (pd.to_numeric(s.dropna().astype(str).str.replace('%', ''), errors='coerce').dropna().between(0, 1).all() or pd.to_numeric(s.dropna().astype(str).str.replace('%', ''), errors='coerce').dropna().between(0, 100).all()) or pd.to_numeric(s.dropna().astype(str).str.replace('%', ''), errors='coerce').notna().all() and s.dropna().astype(str).str.replace('%', '').str.replace(',', '.').str.match(r'^\\d{1,3}(\\.\\d{1,3})?$').all()\n",
        "        },\n",
        "        'Float (Waehrung)': {\n",
        "            'keywords': {'preis','price', 'kosten', 'betrag', 'revenue', 'dollar', 'euro', 'yen', 'usd', 'eur', 'fare','chf', 'gbp', 'sek', 'jpy', '‚Ç¨', '¬£', '$'},\n",
        "            'validation_func': lambda s: (pd.api.types.is_numeric_dtype(s.dropna()) or pd.to_numeric(s.dropna().astype(str).str.replace(',', '.'), errors='coerce').notna().all()) and s.dropna().nunique() > 2\n",
        "        },\n",
        "        'Integer': {\n",
        "            'keywords': {'nb','anzahl', 'menge', 'stueck', 'stk', 'count', 'qty', 'seats', 'rooms', 'nights', 'bags', 'clicks', 'nummer', 'nr', 'quantity', 'val', 'rating','years_as_customer'},\n",
        "            'validation_func': lambda s: pd.to_numeric(s.dropna(), errors='coerce').notna().all() and (pd.to_numeric(s.dropna(), errors='coerce').dropna().apply(lambda x: x.is_integer() if isinstance(x, float) else True).all())\n",
        "        }\n",
        "    }\n",
        "\n",
        "    results: List[Dict[str, str]] = []\n",
        "\n",
        "    hint_categories = [SEMANTIC_HINTS_PRIORITY, SEMANTIC_HINTS_TEXT, SEMANTIC_HINTS_NUMERIC]\n",
        "    SEMANTIC_HINTS_NUMERIC_ORDERED: List[str] = ['Boolean', 'Float (Geografisch)', 'Float (Prozentsatz)', 'Float (Waehrung)', 'Integer']\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
        "        warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "        for column in df.columns:\n",
        "            original_dtype: str = str(df[column].dtype)\n",
        "            semantic_type: str = original_dtype\n",
        "            column_lower: str = column.lower()\n",
        "\n",
        "            found_match: bool = False\n",
        "\n",
        "            for hint_group in hint_categories:\n",
        "                if found_match:\n",
        "                    break\n",
        "                if hint_group is SEMANTIC_HINTS_NUMERIC:\n",
        "                    for sem_type in SEMANTIC_HINTS_NUMERIC_ORDERED:\n",
        "                        hints = hint_group[sem_type]\n",
        "                        name_match = any(keyword in column_lower for keyword in hints['keywords'])\n",
        "                        content_valid = False\n",
        "                        try:\n",
        "                            content_valid = hints['validation_func'](df[column])\n",
        "                        except Exception:\n",
        "                            pass\n",
        "\n",
        "                        if name_match and content_valid:\n",
        "                            semantic_type = sem_type\n",
        "                            found_match = True\n",
        "                            break\n",
        "                else:\n",
        "                    for sem_type, hints in hint_group.items():\n",
        "                        name_match = any(keyword in column_lower for keyword in hints['keywords'])\n",
        "                        content_valid = False\n",
        "                        try:\n",
        "                            content_valid = hints['validation_func'](df[column])\n",
        "                        except Exception:\n",
        "                            pass\n",
        "\n",
        "                        if name_match and content_valid:\n",
        "                            semantic_type = sem_type\n",
        "                            found_match = True\n",
        "                            break\n",
        "\n",
        "            cleaning_muster = generate_cleaning_muster(column, semantic_type)\n",
        "\n",
        "            results.append({\n",
        "                'Spalte': column,\n",
        "                'Urspr√ºnglicher Datentyp': original_dtype,\n",
        "                'Semantischer Typ': semantic_type,\n",
        "                'Bereinigungsmuster': cleaning_muster\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Analyse des DataFrames\n",
        "df_sem_types_v3 = analyze_semantic_type_v3(df)\n",
        "print('*' * 50)\n",
        "# =============================\n",
        "Print(df_sem_types_v3)\n",
        "print('*' * 50)"
      ],
      "metadata": {
        "id": "2xxk-olFgk37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SEMANTISCHE MUSTER-ERKENNUNG V3 mit Bereinigungs vorschlag\n",
        "# ============================================================\n",
        "\n",
        "print('=' * 10, 'SEMANTISCHE MUSTER-ERKENNUNG', '=' * 10)\n",
        "\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from typing import Dict, Any, List, Callable, Union\n",
        "\n",
        "def generate_cleaning_muster(column: str, semantic_type: str) -> str:\n",
        "    \"\"\"\n",
        "    Generiert einen Python-Muster-Vorschlag zur Datenbereinigung basierend auf dem\n",
        "    erkannten semantischen Typ.\n",
        "\n",
        "    Args:\n",
        "        column (str): Der Name der Spalte.\n",
        "        semantic_type (str): Der erkannte semantische Typ.\n",
        "\n",
        "    Returns:\n",
        "        str: Ein String, der den vorgeschlagenen Bereinigungsmuster enth√§lt.\n",
        "    \"\"\"\n",
        "    if semantic_type == 'Datum/Zeit':\n",
        "        return f\"df['{column}'] = pd.to_datetime(df['{column}'], errors='coerce')\"\n",
        "    elif semantic_type == 'ID':\n",
        "        return f\"df['{column}'] = df['{column}'].astype('object')\"\n",
        "    elif semantic_type == 'Boolean':\n",
        "        return f\"df['{column}'] = df['{column}'].astype(bool)\"\n",
        "    elif semantic_type == 'Integer':\n",
        "        return f\"df['{column}'] = pd.to_numeric(df['{column}'], errors='coerce').astype('Int64')\"\n",
        "    elif semantic_type == 'Float (Geografisch)' or semantic_type == 'Float (Waehrung)' or semantic_type == 'Float (Prozentsatz)':\n",
        "        return f\"df['{column}'] = pd.to_numeric(df['{column}'].str.replace(',', '.'), errors='coerce')\"\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def analyze_semantic_type_v3(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Analysiert die semantischen Datentypen der Spalten in einem DataFrame mit\n",
        "    einer angepassten Logik, bei der Spaltenname und Inhalt f√ºr die\n",
        "    semantische Klassifizierung √ºbereinstimmen m√ºssen.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Der zu analysierende DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Ein DataFrame, der die Spalte, den urspr√ºnglichen\n",
        "        Datentyp, den erkannten semantischen Typ und einen\n",
        "        Bereinigungsvorschlag enth√§lt.\n",
        "    \"\"\"\n",
        "    SEMANTIC_HINTS_PRIORITY: Dict[str, Dict[str, Union[set, Callable]]] = {\n",
        "        'ID': {\n",
        "            'keywords': {'id', 'session_id', 'trip_id', 'user_id', 'unique_id', 'kundennummer', 'bestellnr', 'order_id', 'artikelnummer'},\n",
        "            'validation_func': lambda s: ((s.dropna().astype(str).apply(len) >= 5).any())\n",
        "        },\n",
        "        'Datum/Zeit': {\n",
        "            'keywords': {'week','datum', 'zeit', 'date', 'time', 'start', 'end', 'birthdate', 'signup_date', 'check_in', 'check_out', 'departure', 'return', 'geburtstag', 'timestamp', 'creation_date', 'modified_date', 'erstellt'},\n",
        "            'validation_func': lambda s: (pd.to_datetime(s.dropna(), errors='coerce').notna().all() or (s.dropna().astype(str).str.contains(r'[-_/]', na=False).any() and s.dropna().astype(str).str.contains(r'\\d{4}', na=False).any()))\n",
        "        },\n",
        "        'Geometrisch': {\n",
        "            'keywords': {'geom', 'geometry', 'shape', 'wkt', 'geojson', 'coordinates', 'location_data'},\n",
        "            'validation_func': lambda s: (s.dropna().astype(str).str.contains(r'^(POINT|LINESTRING|POLYGON|MULTIPOINT|MULTILINESTRING|MULTIPOLYGON)\\s*\\(', regex=True, na=False).any() or s.dropna().astype(str).str.contains(r'{\"type\":\\s*\"(Point|LineString|Polygon|MultiPoint|MultiLineString|MultiPolygon)\"', regex=True, na=False).any())\n",
        "        },\n",
        "    }\n",
        "    SEMANTIC_HINTS_TEXT: Dict[str, Dict[str, Union[set, Callable]]] = {\n",
        "        'Text (Kategorisch)': {\n",
        "            'keywords': {'city', 'country', 'l√§nder', 'region', 'state', 'bundesland', 'zip', 'plz'},\n",
        "            'validation_func': lambda s: s.dropna().nunique() >= 2 and (pd.api.types.is_string_dtype(s.dropna()) or isinstance(s.dropna().dtype, pd.CategoricalDtype))\n",
        "        },\n",
        "        'Text (Gender)': {\n",
        "            'keywords': {'geschlecht', 'typ', 'category', 'art', 'gender','method'},\n",
        "            'validation_func': lambda s: s.dropna().nunique() >= 2 and (pd.api.types.is_string_dtype(s.dropna()) or isinstance(s.dropna().dtype, pd.CategoricalDtype))\n",
        "        },\n",
        "        'Text (object)': {\n",
        "            'keywords': {'airport', 'destination', 'origin', 'heimat', 'status'},\n",
        "            'validation_func': lambda s: s.dropna().nunique() >= 2 and (pd.api.types.is_string_dtype(s.dropna()) or isinstance(s.dropna().dtype, pd.CategoricalDtype))\n",
        "        },\n",
        "        'Text (Freitext)': {\n",
        "            'keywords': {'name', 'hotel', 'airline', 'beschreibung', 'kommentar', 'nachricht', 'adresse'},\n",
        "            'validation_func': lambda s: pd.api.types.is_string_dtype(s.dropna()) or isinstance(s.dropna().dtype, pd.CategoricalDtype)\n",
        "        },\n",
        "    }\n",
        "    SEMANTIC_HINTS_NUMERIC: Dict[str, Dict[str, Union[set, Callable]]] = {\n",
        "        'Boolean': {\n",
        "            'keywords': {'boolean', 'bool', 'booked', 'married', 'cancellation', 'children','discount'},\n",
        "            'validation_func': lambda s: (s.dropna().nunique() == 2) and (pd.api.types.is_bool_dtype(s.dropna()) or set(s.dropna().astype(str).str.lower().str.strip().unique()).issubset({'true', 'false', '1', '0', 'ja', 'nein', 'yes', 'no', 't', 'f', 'wahr', 'falsch'}))\n",
        "        },\n",
        "        'Float (Geografisch)': {\n",
        "            'keywords': {'lat', 'lon', 'latitude', 'longitude'},\n",
        "            'validation_func': lambda s: pd.to_numeric(s.dropna(), errors='coerce').notna().all() and (pd.to_numeric(s.dropna(), errors='coerce').astype(str).str.count(r'\\.').all() or pd.api.types.is_float_dtype(s.dropna()))\n",
        "        },\n",
        "        'Float (Prozentsatz)': {\n",
        "            'keywords': {'percent', 'pct', 'rate', 'discount', '%'},\n",
        "            'validation_func': lambda s: (pd.to_numeric(s.dropna().astype(str).str.replace('%', ''), errors='coerce').dropna().between(0, 1).all() or pd.to_numeric(s.dropna().astype(str).str.replace('%', ''), errors='coerce').dropna().between(0, 100).all()) or pd.to_numeric(s.dropna().astype(str).str.replace('%', ''), errors='coerce').notna().all() and s.dropna().astype(str).str.replace('%', '').str.replace(',', '.').str.match(r'^\\d{1,3}(\\.\\d{1,3})?$').all()\n",
        "        },\n",
        "        'Float (Waehrung)': {\n",
        "            'keywords': {'preis','price', 'kosten', 'betrag', 'revenue', 'dollar', 'euro', 'yen', 'usd', 'eur', 'fare','chf', 'gbp', 'sek', 'jpy', '‚Ç¨', '¬£', '$'},\n",
        "            'validation_func': lambda s: (pd.api.types.is_numeric_dtype(s.dropna()) or pd.to_numeric(s.dropna().astype(str).str.replace(',', '.'), errors='coerce').notna().all()) and s.dropna().nunique() > 2\n",
        "        },\n",
        "        'Integer': {\n",
        "            'keywords': {'nb','anzahl', 'menge', 'stueck', 'stk', 'count', 'qty', 'seats', 'rooms', 'nights', 'bags', 'clicks', 'nummer', 'nr', 'quantity', 'val', 'rating','years_as_customer'},\n",
        "            'validation_func': lambda s: pd.to_numeric(s.dropna(), errors='coerce').notna().all() and (pd.to_numeric(s.dropna(), errors='coerce').dropna().apply(lambda x: x.is_integer() if isinstance(x, float) else True).all())\n",
        "        }\n",
        "    }\n",
        "\n",
        "    results: List[Dict[str, str]] = []\n",
        "\n",
        "    hint_categories = [SEMANTIC_HINTS_PRIORITY, SEMANTIC_HINTS_TEXT, SEMANTIC_HINTS_NUMERIC]\n",
        "    SEMANTIC_HINTS_NUMERIC_ORDERED: List[str] = ['Boolean', 'Float (Geografisch)', 'Float (Prozentsatz)', 'Float (Waehrung)', 'Integer']\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
        "        warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "        for column in df.columns:\n",
        "            original_dtype: str = str(df[column].dtype)\n",
        "            semantic_type: str = original_dtype\n",
        "            column_lower: str = column.lower()\n",
        "\n",
        "            found_match: bool = False\n",
        "\n",
        "            for hint_group in hint_categories:\n",
        "                if found_match:\n",
        "                    break\n",
        "                if hint_group is SEMANTIC_HINTS_NUMERIC:\n",
        "                    for sem_type in SEMANTIC_HINTS_NUMERIC_ORDERED:\n",
        "                        hints = hint_group[sem_type]\n",
        "                        name_match = any(keyword in column_lower for keyword in hints['keywords'])\n",
        "                        content_valid = False\n",
        "                        try:\n",
        "                            content_valid = hints['validation_func'](df[column])\n",
        "                        except Exception:\n",
        "                            pass\n",
        "\n",
        "                        if name_match and content_valid:\n",
        "                            semantic_type = sem_type\n",
        "                            found_match = True\n",
        "                            break\n",
        "                else:\n",
        "                    for sem_type, hints in hint_group.items():\n",
        "                        name_match = any(keyword in column_lower for keyword in hints['keywords'])\n",
        "                        content_valid = False\n",
        "                        try:\n",
        "                            content_valid = hints['validation_func'](df[column])\n",
        "                        except Exception:\n",
        "                            pass\n",
        "\n",
        "                        if name_match and content_valid:\n",
        "                            semantic_type = sem_type\n",
        "                            found_match = True\n",
        "                            break\n",
        "\n",
        "            cleaning_muster = generate_cleaning_muster(column, semantic_type)\n",
        "\n",
        "            results.append({\n",
        "                'Spalte': column,\n",
        "                'Semantischer Typ': semantic_type,\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Analyse des DataFrames\n",
        "df_sem_types_v3 = analyze_semantic_type_v3(df)\n",
        "print('*' * 50)\n",
        "# =============================\n",
        "print ( df_sem_types_v3)"
      ],
      "metadata": {
        "id": "hl_Nof0ffV6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## √úbersicht mit SAMANTISCHE Typen der Daten mit KPI werte vor bereinigung"
      ],
      "metadata": {
        "id": "89o8gVjoeo3a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import warnings\n",
        "from typing import List, Dict, Union, Callable\n",
        "\n",
        "# --- Anzeigeeinstellungen f√ºr Pandas ---\n",
        "pd.set_option('display.width', 1000)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# ============================================================\n",
        "#  DATA VALIDATION & BUSINESS METRICS (Version, compact)\n",
        "# ============================================================\n",
        "\n",
        "# --- 0. Semantische Analyse-Funktion ---\n",
        "\n",
        "def get_semantic_type(series: pd.Series) -> str:\n",
        "    \"\"\"Klassifiziert eine Series basierend auf Namen und Inhalt.\"\"\"\n",
        "    column_lower = series.name.lower()\n",
        "    semantic_type: str = \"Unbekannt\"\n",
        "    found_match: bool = False\n",
        "\n",
        "    SEMANTIC_HINTS_PRIORITY: Dict[str, Dict[str, Union[set, Callable]]] = {\n",
        "        'ID': {\n",
        "            'keywords': {'id', 'session_id', 'trip_id', 'user_id', 'unique_id', 'kundennummer', 'bestellnr', 'order_id', 'artikelnummer'},\n",
        "            'validation_func': lambda s: ((s.dropna().astype(str).apply(len) >= 5).any())\n",
        "        },\n",
        "        'Datum/Zeit': {\n",
        "            'keywords': {'week','datum', 'zeit', 'date', 'time', 'start', 'end', 'birthdate', 'signup_date', 'check_in', 'check_out', 'departure', 'return', 'geburtstag', 'timestamp', 'creation_date', 'modified_date', 'erstellt'},\n",
        "            'validation_func': lambda s: (pd.to_datetime(s.dropna(), errors='coerce').notna().all() or (s.dropna().astype(str).str.contains(r'[-_/]', na=False).any() and s.dropna().astype(str).str.contains(r'\\d{4}', na=False).any()))\n",
        "        },\n",
        "        'Geometrisch': {\n",
        "            'keywords': {'geom', 'geometry', 'shape', 'wkt', 'geojson', 'coordinates', 'location_data'},\n",
        "            'validation_func': lambda s: (s.dropna().astype(str).str.contains(r'^(POINT|LINESTRING|POLYGON|MULTIPOINT|MULTILINESTRING|MULTIPOLYGON)\\s*\\(', regex=True, na=False).any() or s.dropna().astype(str).str.contains(r'{\"type\":\\s*\"(Point|LineString|Polygon|MultiPoint|MultiLineString|MultiPolygon)\"', regex=True, na=False).any())\n",
        "        },\n",
        "    }\n",
        "    SEMANTIC_HINTS_TEXT: Dict[str, Dict[str, Union[set, Callable]]] = {\n",
        "        'Text (Kategorisch)': {\n",
        "            'keywords': {'city', 'country', 'l√§nder', 'region', 'state', 'bundesland', 'zip', 'plz'},\n",
        "            'validation_func': lambda s: s.dropna().nunique() >= 2 and (pd.api.types.is_string_dtype(s.dropna()) or isinstance(s.dropna().dtype, pd.CategoricalDtype))\n",
        "        },\n",
        "        'Text (Gender)': {\n",
        "            'keywords': {'geschlecht', 'typ', 'category', 'art', 'gender','method'},\n",
        "            'validation_func': lambda s: s.dropna().nunique() >= 2 and (pd.api.types.is_string_dtype(s.dropna()) or isinstance(s.dropna().dtype, pd.CategoricalDtype))\n",
        "        },\n",
        "        'Text (object)': {\n",
        "            'keywords': {'airport', 'destination', 'origin', 'heimat', 'status'},\n",
        "            'validation_func': lambda s: s.dropna().nunique() >= 2 and (pd.api.types.is_string_dtype(s.dropna()) or isinstance(s.dropna().dtype, pd.CategoricalDtype))\n",
        "        },\n",
        "        'Text (Freitext)': {\n",
        "            'keywords': {'name', 'hotel', 'airline', 'beschreibung', 'kommentar', 'nachricht', 'adresse'},\n",
        "            'validation_func': lambda s: pd.api.types.is_string_dtype(s.dropna()) or isinstance(s.dropna().dtype, pd.CategoricalDtype)\n",
        "        },\n",
        "    }\n",
        "    SEMANTIC_HINTS_NUMERIC: Dict[str, Dict[str, Union[set, Callable]]] = {\n",
        "        'Boolean': {\n",
        "            'keywords': {'boolean', 'bool', 'booked', 'married', 'cancellation', 'children','discount'},\n",
        "            'validation_func': lambda s: (s.dropna().nunique() == 2) and (pd.api.types.is_bool_dtype(s.dropna()) or set(s.dropna().astype(str).str.lower().str.strip().unique()).issubset({'true', 'false', '1', '0', 'ja', 'nein', 'yes', 'no', 't', 'f', 'wahr', 'falsch'}))\n",
        "        },\n",
        "        'Float (Geografisch)': {\n",
        "            'keywords': {'lat', 'lon', 'latitude', 'longitude'},\n",
        "            'validation_func': lambda s: pd.to_numeric(s.dropna(), errors='coerce').notna().all() and (pd.to_numeric(s.dropna(), errors='coerce').astype(str).str.count(r'\\.').all() or pd.api.types.is_float_dtype(s.dropna()))\n",
        "        },\n",
        "        'Float (Prozentsatz)': {\n",
        "            'keywords': {'percent', 'pct', 'rate', 'discount', '%'},\n",
        "            'validation_func': lambda s: (pd.to_numeric(s.dropna().astype(str).str.replace('%', ''), errors='coerce').dropna().between(0, 1).all() or pd.to_numeric(s.dropna().astype(str).str.replace('%', ''), errors='coerce').dropna().between(0, 100).all()) or pd.to_numeric(s.dropna().astype(str).str.replace('%', ''), errors='coerce').notna().all() and s.dropna().astype(str).str.replace('%', '').str.replace(',', '.').str.match(r'^\\d{1,3}(\\.\\d{1,3})?$').all()\n",
        "        },\n",
        "        'Float (Waehrung)': {\n",
        "            'keywords': {'preis','price', 'kosten', 'betrag', 'revenue', 'dollar', 'euro', 'yen', 'usd', 'eur', 'fare','chf', 'gbp', 'sek', 'jpy', '‚Ç¨', '¬£', '$'},\n",
        "            'validation_func': lambda s: (pd.api.types.is_numeric_dtype(s.dropna()) or pd.to_numeric(s.dropna().astype(str).str.replace(',', '.'), errors='coerce').notna().all()) and s.dropna().nunique() > 2\n",
        "        },\n",
        "        'Integer': {\n",
        "            'keywords': {'nb','anzahl', 'menge', 'stueck', 'stk', 'count', 'qty', 'seats', 'rooms', 'nights', 'bags', 'clicks', 'nummer', 'nr', 'quantity', 'val', 'rating','years_as_customer'},\n",
        "            'validation_func': lambda s: pd.to_numeric(s.dropna(), errors='coerce').notna().all() and (pd.to_numeric(s.dropna(), errors='coerce').dropna().apply(lambda x: x.is_integer() if isinstance(x, float) else True).all())\n",
        "        }\n",
        "    }\n",
        "\n",
        "    hint_categories = [SEMANTIC_HINTS_PRIORITY, SEMANTIC_HINTS_TEXT, SEMANTIC_HINTS_NUMERIC]\n",
        "    SEMANTIC_HINTS_NUMERIC_ORDERED: List[str] = ['Boolean', 'Float (Geografisch)', 'Float (Prozentsatz)', 'Float (Waehrung)', 'Integer']\n",
        "\n",
        "    with warnings.catch_warnings():\n",
        "        warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
        "        warnings.simplefilter(\"ignore\", UserWarning)\n",
        "\n",
        "        for hint_group in hint_categories:\n",
        "            if found_match:\n",
        "                break\n",
        "            if hint_group is SEMANTIC_HINTS_NUMERIC:\n",
        "                for sem_type in SEMANTIC_HINTS_NUMERIC_ORDERED:\n",
        "                    hints = hint_group[sem_type]\n",
        "                    name_match = any(keyword in column_lower for keyword in hints['keywords'])\n",
        "                    content_valid = False\n",
        "                    try:\n",
        "                        content_valid = hints['validation_func'](series)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                    if name_match and content_valid:\n",
        "                        semantic_type = sem_type\n",
        "                        found_match = True\n",
        "                        break\n",
        "            else:\n",
        "                for sem_type, hints in hint_group.items():\n",
        "                    name_match = any(keyword in column_lower for keyword in hints['keywords'])\n",
        "                    content_valid = False\n",
        "                    try:\n",
        "                        content_valid = hints['validation_func'](series)\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                    if name_match and content_valid:\n",
        "                        semantic_type = sem_type\n",
        "                        found_match = True\n",
        "                        break\n",
        "    return semantic_type\n",
        "\n",
        "# --- 1. Overview ---\n",
        "num_rows, num_cols = df.shape\n",
        "\n",
        "# --- 2. Missing Values ---\n",
        "null_summary = df.isnull().sum().to_frame(\"Null Count\")\n",
        "null_summary[\"Null %\"] = (df.isnull().sum() / len(df) * 100).round(2)\n",
        "\n",
        "# --- 3. Special Characters ---\n",
        "def count_special_chars(text: Union[str, any]) -> int:\n",
        "    if isinstance(text, str):\n",
        "        return len(re.findall(r'[^a-zA-Z0-9 ]', text))\n",
        "    return 0\n",
        "special_chars = df.applymap(count_special_chars).sum().to_frame(\"Special Chars\")\n",
        "\n",
        "# --- 4. Outlier Detection (IQR) ---\n",
        "outlier_counts = {}\n",
        "for col in df.select_dtypes(include=[np.number]).columns:\n",
        "    Q1, Q3 = df[col].quantile([0.25, 0.75])\n",
        "    IQR = Q3 - Q1\n",
        "    outlier_counts[col] = ((df[col] < (Q1 - 1.5*IQR)) | (df[col] > (Q3 + 1.5*IQR))).sum()\n",
        "outliers = pd.DataFrame.from_dict(outlier_counts, orient=\"index\", columns=[\"Outliers\"])\n",
        "\n",
        "# --- 5. Duplicates ---\n",
        "duplicates = df.duplicated().sum()\n",
        "\n",
        "# --- 6. Business Logic (Unit Price) ---\n",
        "df[\"unit_price\"] = df[\"revenue\"] / df[\"nb_sold\"]\n",
        "unit_price_stats = df[\"unit_price\"].describe().round(2)\n",
        "\n",
        "# --- 7. Zusammenfassung: Data Validation Table ---\n",
        "validation_table = pd.concat([null_summary, special_chars, outliers], axis=1).fillna(\"-\")\n",
        "validation_table[\"Datatype\"] = df.dtypes.astype(str)\n",
        "validation_table.reset_index(inplace=True)\n",
        "validation_table.rename(columns={\"index\": \"Column\"}, inplace=True)\n",
        "\n",
        "# Anwenden der semantischen Analyse und Hinzuf√ºgen der Spalte\n",
        "validation_table[\"Semantischer Typ\"] = validation_table[\"Column\"].apply(lambda col: get_semantic_type(df[col]))\n",
        "\n",
        "print(\"============== DATA VALIDATION REPORT ==============\")\n",
        "print(f\"Dataset Shape: {num_rows:,} rows √ó {num_cols} columns\")\n",
        "print(f\"Duplicates: {duplicates}\")\n",
        "print(\"\\nValidation Table:\")\n",
        "print(validation_table)\n",
        "print(\"\\nUnit Price Check:\")\n",
        "print(unit_price_stats)\n",
        "\n",
        "# ============================================================\n",
        "# 8. BUSINESS METRICS (KPI Table)\n",
        "# ============================================================\n",
        "\n",
        "rev_per_customer = df.groupby(\"customer_id\")[\"revenue\"].sum().mean()\n",
        "avg_conversion = (df[\"nb_sold\"] / df[\"nb_site_visits\"]).mean()\n",
        "retention_rate = (df[\"years_as_customer\"] > 1).mean()\n",
        "\n",
        "kpi_table = pd.DataFrame({\n",
        "    \"KPI\": [\"Avg Revenue per Customer\", \"Conversion Rate\", \"Retention Rate (>1y)\"],\n",
        "    \"Value\": [f\"{rev_per_customer:.2f}\", f\"{avg_conversion:.2%}\", f\"{retention_rate:.2%}\"]\n",
        "})\n",
        "\n",
        "print(\"\\n============== BUSINESS KPI REPORT ==============\")\n",
        "print(kpi_table)\n",
        "\n",
        "# ============================================================\n",
        "# 9. Visualization\n",
        "# ============================================================\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.lineplot(data=df, x=\"week\", y=\"revenue\", estimator=\"mean\", ci=None)\n",
        "plt.title(\"Average Weekly Revenue Trend\")\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.barplot(x=\"sales_method\", y=\"revenue\", data=df, estimator=\"mean\", ci=None)\n",
        "plt.title(\"Revenue by Sales Method\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XkreXCVfeXun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scater PLOT mit Drop Down Object"
      ],
      "metadata": {
        "id": "I0PLL6xDCN2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================================\n",
        "# üìä ALLGEMEINE  ScaterPlot\n",
        "# =======================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from typing import List, Dict, Union\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Deaktiviere zuk√ºnftige Pandas-Warnungen, die bei der Typkonvertierung auftreten k√∂nnen\n",
        "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
        "\n",
        "# =======================================================\n",
        "# ====== 1. Spaltenklassifizierung nach Typ ======\n",
        "# =======================================================\n",
        "def classify_columns_by_type(df: pd.DataFrame) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Klassifiziert die Spalten eines DataFrame rein basierend auf ihrem Datentyp.\n",
        "\n",
        "    Args:\n",
        "        df: Das zu klassifizierende Pandas DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        Ein Dictionary, das Spaltentypen auf eine Liste der zugeh√∂rigen Spaltennamen abbildet.\n",
        "    \"\"\"\n",
        "    categorical_columns: List[str] = []\n",
        "    numerical_columns: List[str] = []\n",
        "\n",
        "    for column in df.columns:\n",
        "        col_dtype = df[column].dtype\n",
        "\n",
        "        if pd.api.types.is_numeric_dtype(col_dtype):\n",
        "            numerical_columns.append(column)\n",
        "        elif pd.api.types.is_object_dtype(col_dtype) or pd.api.types.is_categorical_dtype(col_dtype):\n",
        "            categorical_columns.append(column)\n",
        "\n",
        "    return {\n",
        "        'categorical': categorical_columns,\n",
        "        'numerical': numerical_columns,\n",
        "    }\n",
        "\n",
        "\n",
        "# =======================================================\n",
        "# ====== 2. Interaktive Benutzeroberfl√§che mit ipywidgets ======\n",
        "# =======================================================\n",
        "\n",
        "# HINWEIS: F√ºge hier deinen DataFrame ein, z.B. durch Laden aus einer CSV-Datei\n",
        "# Beispiel: df = pd.read_csv('deine_daten.csv')\n",
        "\n",
        "# --- HIER MUSST DU DEINEN DATAFRAME `df` DEFINIEREN ---\n",
        "\n",
        "# Klassifizieren der Spalten des DataFrames mit der neuen Funktion\n",
        "classified_cols = classify_columns_by_type(df)\n",
        "\n",
        "# Dropdown-Men√ºs f√ºr die Spaltenauswahl erstellen\n",
        "# Verwende die klassifizierten numerischen Spalten f√ºr die PCA-Features (X und Y)\n",
        "pca_cols_options = classified_cols['numerical']\n",
        "# Verwende die klassifizierten kategorialen Spalten f√ºr die Zielvariable (Farbe)\n",
        "target_col_options = classified_cols['categorical']\n",
        "\n",
        "pca_dropdown_1 = widgets.Dropdown(\n",
        "    options=pca_cols_options,\n",
        "    description='y (numerisch):'\n",
        ")\n",
        "pca_dropdown_2 = widgets.Dropdown(\n",
        "    options=pca_cols_options,\n",
        "    description='X (numerisch):'\n",
        ")\n",
        "target_dropdown = widgets.Dropdown(\n",
        "    options=target_col_options,\n",
        "    description='Verteilung (Object):'\n",
        ")\n",
        "\n",
        "button = widgets.Button(\n",
        "    description='PCA Plot erstellen',\n",
        "    button_style='success',\n",
        "    tooltip='Klicken Sie, um den PCA-Plot mit den ausgew√§hlten Spalten zu erstellen'\n",
        ")\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    with output:\n",
        "        clear_output(wait=True)\n",
        "        try:\n",
        "            # Manuelle Auswahl der Features und der Zielvariable\n",
        "            selected_pca_features = [pca_dropdown_1.value, pca_dropdown_2.value]\n",
        "            selected_target_col = target_dropdown.value\n",
        "\n",
        "            # Daten vorbereiten: W√§hle NUR die vom Nutzer ausgew√§hlten Spalten\n",
        "            df_cleaned = df.dropna(subset=[selected_target_col])\n",
        "\n",
        "            # W√§hle nur die relevanten numerischen Spalten aus\n",
        "            numerical_features = [col for col in selected_pca_features if col in classified_cols['numerical']]\n",
        "\n",
        "            # √úberpr√ºfen, ob numerische Features ausgew√§hlt wurden\n",
        "            if not numerical_features:\n",
        "                print(\"Bitte w√§hlen Sie mindestens eine numerische Spalte aus.\")\n",
        "                return\n",
        "\n",
        "            X = df_cleaned[numerical_features].values\n",
        "            y = df_cleaned[selected_target_col].values\n",
        "\n",
        "            # Imputation der fehlenden Werte mit dem Mittelwert\n",
        "            imputer = SimpleImputer(strategy='mean')\n",
        "            X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "            # Skalieren der Daten\n",
        "            scaler = StandardScaler()\n",
        "            X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "            # PCA auf die skalierten Daten anwenden\n",
        "            # Wichtig: n_components auf die Anzahl der ausgew√§hlten Features setzen,\n",
        "            # oder den kleineren Wert zwischen n_features und 2\n",
        "            n_components = min(2, len(numerical_features))\n",
        "            pca = PCA(n_components=n_components, random_state=42)\n",
        "            X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "            # Visualisierung\n",
        "            plt.figure(figsize=(10, 8))\n",
        "\n",
        "            # W√§hle die ersten beiden PCA-Komponenten f√ºr die Achsen,\n",
        "            # da die PCA-Transformation auf die ausgew√§hlten Features angewendet wird\n",
        "            sns.scatterplot(\n",
        "                x=X_pca[:, 0],\n",
        "                y=X_pca[:, 1],\n",
        "                hue=y,\n",
        "                palette=\"tab10\"\n",
        "            )\n",
        "            plt.title(f\"PCA Projektion auf ausgew√§hlte Spalten: {', '.join(numerical_features)}\", fontsize=15)\n",
        "            plt.xlabel(\"PC1\", fontsize=12)\n",
        "            plt.ylabel(\"PC2\", fontsize=12)\n",
        "            plt.legend(title=selected_target_col, loc='best')\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Ein Fehler ist aufgetreten: {e}\")\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "\n",
        "# Widgets anzeigen\n",
        "print(\"W√§hlen Sie die Spalten f√ºr die PCA-Visualisierung aus:\")\n",
        "display(widgets.VBox([\n",
        "    pca_dropdown_1,\n",
        "    pca_dropdown_2,\n",
        "    target_dropdown,\n",
        "    button,\n",
        "    output\n",
        "]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "KhCqcbr0BvL7",
        "outputId": "b3ca6c09-c794-467c-c7b6-a7c1416d5caf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- üõ†Ô∏è Daten f√ºr das Modell vorbereiten ---\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3263148862.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"--- üõ†Ô∏è Daten f√ºr das Modell vorbereiten ---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mkategorische_spalten\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkategorische_spalten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interaktive PCA 2 DIMENSION Object Drop Down"
      ],
      "metadata": {
        "id": "J4GwlJbmGWDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================================================\n",
        "# üìä ALLGEMEINE  ScaterPlot mit automatische 2 PC verteilung\n",
        "# =======================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from typing import List, Dict, Union\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Deaktiviere zuk√ºnftige Pandas-Warnungen, die bei der Typkonvertierung auftreten k√∂nnen\n",
        "warnings.simplefilter(action='ignore', category=pd.errors.SettingWithCopyWarning)\n",
        "\n",
        "# =======================================================\n",
        "# ====== 1. Spaltenklassifizierung nach Datentyp ======\n",
        "# =======================================================\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "\n",
        "def classify_columns_by_type(df: pd.DataFrame) -> Dict[str, List[str]]:\n",
        "    \"\"\"\n",
        "    Klassifiziert die Spalten eines DataFrame rein basierend auf ihrem Datentyp.\n",
        "\n",
        "    Args:\n",
        "        df: Das zu klassifizierende Pandas DataFrame.\n",
        "\n",
        "    Returns:\n",
        "        Ein Dictionary, das Spaltentypen auf eine Liste der zugeh√∂rigen Spaltennamen abbildet.\n",
        "    \"\"\"\n",
        "    categorical_columns: List[str] = []\n",
        "    numerical_columns: List[str] = []\n",
        "\n",
        "    for column in df.columns:\n",
        "        col_dtype = df[column].dtype\n",
        "\n",
        "        if pd.api.types.is_numeric_dtype(col_dtype):\n",
        "            numerical_columns.append(column)\n",
        "        elif pd.api.types.is_object_dtype(col_dtype) or pd.api.types.is_categorical_dtype(col_dtype):\n",
        "            categorical_columns.append(column)\n",
        "\n",
        "    return {\n",
        "        'categorical': categorical_columns,\n",
        "        'numerical': numerical_columns,\n",
        "    }\n",
        "\n",
        "# =======================================================\n",
        "#  2. Interaktive Benutzeroberfl√§che mit ipywidgets\n",
        "# =======================================================\n",
        "\n",
        "# Klassifizieren der Spalten des geladenen Datenrahmens\n",
        "# HINWEIS: Ersetzen Sie 'classify_columns' durch den korrekten Funktionsnamen 'classify_columns_by_type'\n",
        "classified_cols = classify_columns_by_type(df)\n",
        "\n",
        "# Dropdown-Men√º f√ºr die Zielvariable (Farbe) erstellen\n",
        "target_col_options = classified_cols['categorical'] + classified_cols['numerical']\n",
        "\n",
        "target_dropdown = widgets.Dropdown(\n",
        "    options=target_col_options,\n",
        "    description='Zielvariable (Farbe):'\n",
        ")\n",
        "\n",
        "button = widgets.Button(\n",
        "    description='PCA Plot erstellen',\n",
        "    button_style='success',\n",
        "    tooltip='Klicken Sie, um den PCA-Plot zu erstellen'\n",
        ")\n",
        "\n",
        "output = widgets.Output()\n",
        "\n",
        "def on_button_clicked(b):\n",
        "    with output:\n",
        "        clear_output(wait=True)\n",
        "        try:\n",
        "            selected_target_col = target_dropdown.value\n",
        "\n",
        "            numerical_features = classified_cols['numerical']\n",
        "\n",
        "            if len(numerical_features) < 2:\n",
        "                print(\"Es werden mindestens 2 numerische Spalten f√ºr die PCA ben√∂tigt.\")\n",
        "                return\n",
        "\n",
        "            df_cleaned = df.dropna(subset=[selected_target_col])\n",
        "            X = df_cleaned[numerical_features].values\n",
        "            y = df_cleaned[selected_target_col].values\n",
        "\n",
        "            # Imputation der fehlenden Werte\n",
        "            imputer = SimpleImputer(strategy='mean')\n",
        "            X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "            # Skalieren der Daten\n",
        "            scaler = StandardScaler()\n",
        "            X_scaled = scaler.fit_transform(X_imputed)\n",
        "\n",
        "            # PCA auf 2 Komponenten reduzieren\n",
        "            pca = PCA(n_components=2, random_state=42)\n",
        "            X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "            # Visualisierung\n",
        "            plt.figure(figsize=(10, 8))\n",
        "\n",
        "            sns.scatterplot(\n",
        "                x=X_pca[:, 0],\n",
        "                y=X_pca[:, 1],\n",
        "                hue=y,\n",
        "                palette=\"tab10\"\n",
        "            )\n",
        "\n",
        "            plt.title(\"PCA Projektion: PC1 vs. PC2\", fontsize=15)\n",
        "            plt.xlabel(\"PC1\", fontsize=12)\n",
        "            plt.ylabel(\"PC2\", fontsize=12)\n",
        "            plt.legend(title=selected_target_col, loc='best')\n",
        "            plt.grid(True)\n",
        "            plt.show()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Ein Fehler ist aufgetreten: {e}\")\n",
        "\n",
        "button.on_click(on_button_clicked)\n",
        "\n",
        "# Widgets anzeigen\n",
        "print(\"W√§hlen Sie die Spalte zur Einf√§rbung der PCA-Punkte aus:\")\n",
        "display(widgets.VBox([\n",
        "    target_dropdown,\n",
        "    button,\n",
        "    output\n",
        "]))"
      ],
      "metadata": {
        "id": "5DVph7xVGDCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML BEREINIGUNG"
      ],
      "metadata": {
        "id": "tGBNvO-zJEgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ‚öôÔ∏è ML BEREINIGUNG\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Angenommen, das DataFrame 'df' wurde bereits geladen\n",
        "# df = pd.read_csv('vehicle.csv')\n",
        "\n",
        "def handle_outliers_iqr(df):\n",
        "    \"\"\"\n",
        "    Behandelt Ausrei√üer in numerischen Spalten mithilfe der IQR-Methode (Capping).\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Das DataFrame mit den numerischen Spalten.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Das DataFrame mit behandelten Ausrei√üern.\n",
        "    \"\"\"\n",
        "    df_temp = df.copy()\n",
        "    numeric_cols = df_temp.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "    if not numeric_cols:\n",
        "        print(\"‚ÑπÔ∏è Keine numerischen Spalten gefunden, Ausrei√üer-Behandlung √ºbersprungen.\")\n",
        "        return df_temp\n",
        "\n",
        "    print(\"üìä Start der Ausrei√üer-Behandlung (Capping) nach IQR-Methode...\")\n",
        "    for col in numeric_cols:\n",
        "        Q1 = df_temp[col].quantile(0.25)\n",
        "        Q3 = df_temp[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        outliers_count = ((df_temp[col] < lower_bound) | (df_temp[col] > upper_bound)).sum()\n",
        "        if outliers_count > 0:\n",
        "            df_temp[col] = np.where(df_temp[col] < lower_bound, lower_bound, df_temp[col])\n",
        "            df_temp[col] = np.where(df_temp[col] > upper_bound, upper_bound, df_temp[col])\n",
        "            print(f\"‚úÖ {outliers_count} Ausrei√üer in Spalte '{col}' behandelt.\")\n",
        "        else:\n",
        "            print(f\"‚ÑπÔ∏è Keine Ausrei√üer in Spalte '{col}' gefunden.\")\n",
        "    print(\"‚úÖ Ausrei√üer-Behandlung abgeschlossen.\")\n",
        "    return df_temp\n",
        "\n",
        "def preprocess_data(df, target_column_name):\n",
        "    \"\"\"\n",
        "    F√ºhrt die Standardisierung und Kodierung der Zielvariablen durch.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Das zu verarbeitende DataFrame.\n",
        "        target_column_name (str): Der Name der Zielspalte.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame, np.ndarray, sklearn.preprocessing.LabelEncoder:\n",
        "        Die vorbereiteten Features (X), die kodierte Zielvariable (y),\n",
        "        und der LabelEncoder.\n",
        "    \"\"\"\n",
        "    df_temp = df.copy()\n",
        "\n",
        "    # 1. Features und Zielvariable trennen\n",
        "    if target_column_name not in df_temp.columns:\n",
        "        raise ValueError(f\"‚ùå Zielspalte '{target_column_name}' nicht im DataFrame gefunden.\")\n",
        "\n",
        "    y = df_temp[target_column_name]\n",
        "    X = df_temp.drop(columns=[target_column_name])\n",
        "\n",
        "    # 2. Numerische Spalten standardisieren\n",
        "    numeric_cols = X.select_dtypes(include=np.number).columns.tolist()\n",
        "    if numeric_cols:\n",
        "        scaler = StandardScaler()\n",
        "        X[numeric_cols] = scaler.fit_transform(X[numeric_cols])\n",
        "        print(\"‚úÖ Numerische Spalten wurden standardisiert.\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è Keine numerischen Spalten f√ºr die Standardisierung gefunden.\")\n",
        "\n",
        "    # 3. Zielvariable kodieren\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y)\n",
        "    print(\"‚úÖ Zielvariable in numerische Werte kodiert (LabelEncoder).\")\n",
        "    for i, class_name in enumerate(le.classes_):\n",
        "        print(f\"  '{class_name}' -> {i}\")\n",
        "\n",
        "    return X, y_encoded, le\n",
        "\n",
        "# Beispiel der Anwendung\n",
        "# --- AUSF√úHRUNG DER BEHANDLUNG ---\n",
        "print(\"--- ‚öôÔ∏è START DER DATENBEHANDLUNG ---\")\n",
        "df_after_outliers = handle_outliers_iqr(df.copy())\n",
        "X_processed, y_encoded, label_encoder = preprocess_data(df_after_outliers, 'class')\n",
        "\n",
        "print(\"--- ‚úÖ DATEN F√úR MODELLIERUNG VORBEREITET ---\")\n",
        "print(f\"Gr√∂√üe der Feature-Matrix (X): {X_processed.shape}\")\n",
        "print(f\"Gr√∂√üe des Zielvektors (y): {y_encoded.shape}\")\n",
        "print(\"--- üèÅ ENDE DER DATENBEHANDLUNG ---\")"
      ],
      "metadata": {
        "id": "w49b6HxeJCOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML √úberwachtes Lernen  "
      ],
      "metadata": {
        "id": "xdtp9ap_brct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "üìä √úberwachtes Lernen mit ML\n",
        "# =====================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "import sys\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, ConfusionMatrixDisplay\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Optionale Bibliotheken\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    has_xgb = True\n",
        "except ImportError:\n",
        "    has_xgb = False\n",
        "\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier\n",
        "    has_lgbm = True\n",
        "except ImportError:\n",
        "    has_lgbm = False\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier\n",
        "    has_catboost = True\n",
        "except ImportError:\n",
        "    has_catboost = False\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "print(\"Alle notwendigen Bibliotheken erfolgreich importiert!\")\n",
        "print('*' * 50)\n",
        "\n",
        "# ====== DATEN PR√úFEN UND ZUWEISEN ======\n",
        "print(' * * * * * * * * * *', '[DATEN PR√úFEN UND ZUWEISEN]', ' * * * * * * * * * *')\n",
        "# Weist das Haupt-DataFrame (df) der tempor√§ren Variable (df_temp) zu,\n",
        "# um Kompatibilit√§t mit dem bereitgestellten Code zu gew√§hrleisten.\n",
        "df_temp = df\n",
        "\n",
        "# =====================================\n",
        "print('*' * 50)\n",
        "# =======================================================\n",
        "#    1. Spaltenklassifizierung nach Typ tabelarisch\n",
        "# =======================================================\n",
        "def classify_columns_by_type(df: pd.DataFrame):\n",
        "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    return {'categorical': categorical_columns, 'numerical': numerical_columns}\n",
        "\n",
        "# === 1. Features und Zielvariable trennen (mit Dropdown) ===\n",
        "print(\"--- üõ†Ô∏è Daten f√ºr das Modell vorbereiten ---\")\n",
        "classified_cols = classify_columns_by_type(df_temp)\n",
        "kategorische_spalten = classified_cols['categorical']\n",
        "\n",
        "if not kategorische_spalten:\n",
        "    raise ValueError(\"‚ùå Keine kategoriale Spalte im DataFrame gefunden.\")\n",
        "\n",
        "print(\"\\nFolgende Spalten sind vom Typ 'object' oder 'category':\")\n",
        "for i, col in enumerate(kategorische_spalten):\n",
        "    print(f\"  {i+1}. {col}\")\n",
        "\n",
        "# Dropdown-Men√º und OK-Button erstellen\n",
        "ziel_dropdown = widgets.Dropdown(\n",
        "    options=kategorische_spalten,\n",
        "    description='Zielvariable:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "ok_button = widgets.Button(\n",
        "    description='OK',\n",
        "    button_style='success',\n",
        "    tooltip='Ausgew√§hlte Spalte best√§tigen'\n",
        ")\n",
        "\n",
        "output_widget = widgets.Output()\n",
        "\n",
        "def on_ok_button_clicked(b):\n",
        "    with output_widget:\n",
        "        clear_output(wait=True)\n",
        "        try:\n",
        "            zielvariable_name = ziel_dropdown.value\n",
        "\n",
        "            y = df_temp[zielvariable_name]\n",
        "            X = df_temp.drop(columns=[zielvariable_name])\n",
        "\n",
        "            print(f\"\\n‚úÖ Zielvariable: '{y.name}' (Datentyp: {y.dtype})\")\n",
        "            print(f\"Anzahl der Features: {X.shape[1]}\")\n",
        "\n",
        "            # === 2. One-Hot-Encoding f√ºr Features ===\n",
        "            kategorische_spalten_in_X = X.select_dtypes(include=['object', 'category']).columns\n",
        "            if not kategorische_spalten_in_X.empty:\n",
        "                X = pd.get_dummies(X, columns=kategorische_spalten_in_X, drop_first=True)\n",
        "                print(\"\\n‚úÖ Kategorische Spalten in X wurden kodiert (One-Hot-Encoding).\")\n",
        "            else:\n",
        "                print(\"\\n‚ÑπÔ∏è Keine kategorischen Spalten in X gefunden.\")\n",
        "\n",
        "            # === 3. Zielvariable kodieren ===\n",
        "            if y.dtype == 'object' or y.dtype.name == 'category':\n",
        "                le = LabelEncoder()\n",
        "                y_encoded = le.fit_transform(y)\n",
        "                print(\"\\n‚úÖ Zielvariable in numerische Werte kodiert (LabelEncoder).\")\n",
        "                for i, class_name in enumerate(le.classes_):\n",
        "                    print(f\"  '{class_name}' -> {i}\")\n",
        "            else:\n",
        "                le = None\n",
        "                y_encoded = y\n",
        "                print(\"\\n‚ÑπÔ∏è Zielvariable bereits numerisch, keine Kodierung erforderlich.\")\n",
        "\n",
        "            # === 4. Konsistenzpr√ºfung vor Split ===\n",
        "            print(f\"\\n--- Konsistenzpr√ºfung vor Train/Test-Split ---\")\n",
        "            if X.shape[0] != y_encoded.shape[0]:\n",
        "                raise ValueError(f\"‚ùå Inkonsistente Anzahl Samples: X={X.shape[0]}, y={y_encoded.shape[0]}\")\n",
        "            else:\n",
        "                print(\"‚úÖ Shapes konsistent.\")\n",
        "\n",
        "            # === 5. Train/Test-Split ===\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "            print(f\"\\nX_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
        "            print(f\"y_train: {y_train.shape}, y_test: {y_test.shape}\")\n",
        "\n",
        "            # === 6. Modelltraining und Bewertung ===\n",
        "            print(\"\\n--- üß† Modelle trainieren und bewerten ---\")\n",
        "            modelle = {\n",
        "                \"Logistische Regression\": LogisticRegression(max_iter=500),\n",
        "                \"KNN\": KNeighborsClassifier(),\n",
        "                \"Entscheidungsbaum\": DecisionTreeClassifier(),\n",
        "                \"Random Forest\": RandomForestClassifier(),\n",
        "                \"SVM\": SVC(probability=True),\n",
        "                \"Naive Bayes\": GaussianNB(),\n",
        "                \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "                \"AdaBoost\": AdaBoostClassifier(),\n",
        "                \"Neuronales Netz (MLP)\": MLPClassifier(max_iter=500),\n",
        "                \"Extra Trees\": ExtraTreesClassifier(),\n",
        "                \"Bagging\": BaggingClassifier(),\n",
        "                \"SGD\": SGDClassifier(loss=\"log_loss\"),\n",
        "                \"Ridge\": RidgeClassifier()\n",
        "            }\n",
        "\n",
        "            if has_xgb:\n",
        "                modelle[\"XGBoost\"] = XGBClassifier(eval_metric=\"mlogloss\", use_label_encoder=False)\n",
        "            if has_lgbm:\n",
        "                modelle[\"LightGBM\"] = LGBMClassifier(verbose=-1)\n",
        "            if has_catboost:\n",
        "                modelle[\"CatBoost\"] = CatBoostClassifier(verbose=0)\n",
        "\n",
        "            ergebnisse = {}\n",
        "            for name, model in modelle.items():\n",
        "                print(f\"Starte Training f√ºr: {name}...\")\n",
        "                try:\n",
        "                    model.fit(X_train, y_train)\n",
        "                    y_pred = model.predict(X_test)\n",
        "                    acc = accuracy_score(y_test, y_pred)\n",
        "                    prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "                    rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "                    f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "                    roc_auc = np.nan\n",
        "                    if hasattr(model, \"predict_proba\"):\n",
        "                        y_pred_proba = model.predict_proba(X_test)\n",
        "                        if y_pred_proba.shape[1] == 2:\n",
        "                            roc_auc = roc_auc_score(y_test, y_pred_proba[:, 1])\n",
        "                    scores = cross_val_score(model, X, y_encoded, cv=5, scoring='accuracy')\n",
        "                    ergebnisse[name] = {\n",
        "                        'accuracy': acc, 'precision': prec, 'recall': rec, 'f1_score': f1,\n",
        "                        'roc_auc': roc_auc, 'cv_accuracy': np.mean(scores), 'model': model\n",
        "                    }\n",
        "                    print(f\"‚úÖ Training f√ºr {name} abgeschlossen.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Modell {name} konnte nicht trainiert werden: {e}\")\n",
        "\n",
        "            # Sortieren und Filter\n",
        "            ergebnisse = dict(sorted(ergebnisse.items(), key=lambda x: x[1]['cv_accuracy'], reverse=True))\n",
        "            threshold = 0.6\n",
        "            gefilterte_ergebnisse = {name: metrics for name, metrics in ergebnisse.items() if metrics['cv_accuracy'] >= threshold}\n",
        "\n",
        "            # √úbersicht\n",
        "            print(\"\\nüìä Ergebnisse der Modelle (sortiert nach CV-Accuracy):\")\n",
        "            for name, metrics in ergebnisse.items():\n",
        "                roc_auc_display = f\"{metrics['roc_auc']:.3f}\" if not np.isnan(metrics['roc_auc']) else \"N/A\"\n",
        "                print(f\"{name:20s} | Accuracy: {metrics['accuracy']:.3f} | CV-Accuracy: {metrics['cv_accuracy']:.3f} | ROC-AUC: {roc_auc_display}\")\n",
        "\n",
        "            # Bestes Modell ausw√§hlen\n",
        "            if gefilterte_ergebnisse:\n",
        "                best_model_name, best_metrics = next(iter(gefilterte_ergebnisse.items()))\n",
        "                best_model = best_metrics['model']\n",
        "                print(f\"\\nüèÜ Bestes Modell unter den geeigneten: {best_model_name}\")\n",
        "                best_model.fit(X, y_encoded)\n",
        "\n",
        "                # Balkendiagramm\n",
        "                namen = list(gefilterte_ergebnisse.keys())\n",
        "                test_acc = [metrics['accuracy'] for metrics in gefilterte_ergebnisse.values()]\n",
        "                cv_acc = [metrics['cv_accuracy'] for metrics in gefilterte_ergebnisse.values()]\n",
        "                fig, ax = plt.subplots(figsize=(10, 6))\n",
        "                y_pos = np.arange(len(namen))\n",
        "                bar_width = 0.35\n",
        "                ax.barh(y_pos - bar_width/2, test_acc, height=bar_width, label=\"Test-Genauigkeit\", color=\"skyblue\")\n",
        "                ax.barh(y_pos + bar_width/2, cv_acc, height=bar_width, label=\"CV-Genauigkeit\", color=\"orange\")\n",
        "                ax.set_yticks(y_pos)\n",
        "                ax.set_yticklabels(namen)\n",
        "                ax.set_xlim(0.0, 1.0)\n",
        "                ax.set_xlabel(\"Genauigkeit\")\n",
        "                ax.set_title(\"Modellvergleich ‚Äì Klassifikation (CV-Genauigkeit >= 0.6)\")\n",
        "                ax.legend()\n",
        "                for i, v in enumerate(test_acc):\n",
        "                    ax.text(v + 0.01, i - bar_width/2, f\"{v:.2f}\", va=\"center\")\n",
        "                for i, v in enumerate(cv_acc):\n",
        "                    ax.text(v + 0.01, i + bar_width/2, f\"{v:.2f}\", va=\"center\")\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "\n",
        "                # Konfusionsmatrix auf Testdaten\n",
        "                y_pred_test = best_model.predict(X_test)\n",
        "                if le is not None:\n",
        "                    display_labels = le.classes_\n",
        "                else:\n",
        "                    display_labels = np.unique(np.concatenate((y_test, y_pred_test)))\n",
        "                cm = confusion_matrix(y_test, y_pred_test, labels=np.arange(len(display_labels)))\n",
        "                disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
        "                disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
        "                plt.title(f\"Konfusionsmatrix ‚Äì {best_model_name}\")\n",
        "                plt.show()\n",
        "\n",
        "                # --- üìã Liste falsch klassifizierter Zeilen ---\n",
        "                y_pred_test = best_model.predict(X_test)\n",
        "                if le is not None:\n",
        "                    y_test_decoded = le.inverse_transform(y_test)\n",
        "                    y_pred_decoded = le.inverse_transform(y_pred_test)\n",
        "                else:\n",
        "                    y_test_decoded = y_test\n",
        "                    y_pred_decoded = y_pred_test\n",
        "\n",
        "                fehler_df = X_test.copy()\n",
        "                fehler_df[\"Echte_Klasse\"] = y_test_decoded\n",
        "                fehler_df[\"Vorhergesagte_Klasse\"] = y_pred_decoded\n",
        "                fehler_df = fehler_df[fehler_df[\"Echte_Klasse\"] != fehler_df[\"Vorhergesagte_Klasse\"]]\n",
        "\n",
        "                print(\"\\n--- üìã Falsch klassifizierte Zeilen ---\")\n",
        "                if fehler_df.empty:\n",
        "                    print(\"‚úÖ Alle Testdaten korrekt klassifiziert!\")\n",
        "                else:\n",
        "                    print(f\"‚ùå {len(fehler_df)} falsch klassifizierte Zeilen gefunden.\")\n",
        "                    print(fehler_df.head(10))\n",
        "\n",
        "                # --- üìã Vergleich echte Objekte aus df vs. Modellvorhersage ---\n",
        "                y_pred_all = best_model.predict(X)\n",
        "                if le is not None:\n",
        "                    y_true_all = le.inverse_transform(y_encoded)\n",
        "                    y_pred_all = le.inverse_transform(y_pred_all)\n",
        "                else:\n",
        "                    y_true_all = y_encoded\n",
        "                    y_pred_all = y_pred_all\n",
        "\n",
        "                vergleich_df = X.copy()\n",
        "                vergleich_df[\"Echte_Klasse_df\"] = y_true_all\n",
        "                vergleich_df[\"Vorhergesagte_Klasse\"] = y_pred_all\n",
        "                abweichungen_df = vergleich_df[vergleich_df[\"Echte_Klasse_df\"] != vergleich_df[\"Vorhergesagte_Klasse\"]]\n",
        "\n",
        "                print(\"\\n--- üìã Vergleich echte Objekte aus df vs. Modellvorhersage ---\")\n",
        "                if abweichungen_df.empty:\n",
        "                    print(\"‚úÖ Modell sieht alle Objekte gleich wie im Datensatz definiert!\")\n",
        "                else:\n",
        "                    print(f\"‚ùå {len(abweichungen_df)} Abweichungen gefunden.\")\n",
        "                    print(abweichungen_df.head(20))\n",
        "\n",
        "                # Statistik der Abweichungen pro Klasse\n",
        "                if not abweichungen_df.empty:\n",
        "                    gesamt_pro_klasse = pd.Series(y_true_all).value_counts().sort_index()\n",
        "                    fehler_pro_klasse = abweichungen_df[\"Echte_Klasse_df\"].value_counts().sort_index()\n",
        "                    prozent_fehler = (fehler_pro_klasse / gesamt_pro_klasse * 100).round(2)\n",
        "                    klasse_statistik = pd.DataFrame({\n",
        "                        \"Gesamtanzahl\": gesamt_pro_klasse,\n",
        "                        \"Fehleranzahl\": fehler_pro_klasse,\n",
        "                        \"Fehler_%\": prozent_fehler\n",
        "                    }).fillna(0)\n",
        "                    print(\"\\n--- üìä Abweichungen pro Klasse ---\")\n",
        "                    print(klasse_statistik)\n",
        "                else:\n",
        "                    print(\"‚úÖ Keine Abweichungen, alle Objekte korrekt erkannt!\")\n",
        "\n",
        "                print(\"\\n--- üèÅ Modellanalyse abgeschlossen ---\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Ein Fehler ist aufgetreten: {e}\")\n",
        "\n",
        "ok_button.on_click(on_ok_button_clicked)\n",
        "\n",
        "# Widgets anzeigen\n",
        "print(\"\\nBitte w√§hlen Sie die Zielvariable aus und klicken Sie auf OK.\")\n",
        "display(widgets.VBox([\n",
        "    ziel_dropdown,\n",
        "    ok_button,\n",
        "    output_widget\n",
        "]))\n",
        "# =======================================================\n",
        "#       1. Spaltenklassifizierung nach Typ PCA 2D\n",
        "# =======================================================\n",
        "def classify_columns_by_type(df: pd.DataFrame):\n",
        "    categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "    numerical_columns = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    return {'categorical': categorical_columns, 'numerical': numerical_columns}\n",
        "\n",
        "# === 1. Features und Zielvariable trennen (mit Dropdown) ===\n",
        "print(\"--- üõ†Ô∏è Daten f√ºr das Modell vorbereiten PCA 2D---\")\n",
        "classified_cols = classify_columns_by_type(df_temp)\n",
        "kategorische_spalten = classified_cols['categorical']\n",
        "\n",
        "if not kategorische_spalten:\n",
        "    raise ValueError(\"‚ùå Keine kategoriale Spalte im DataFrame gefunden.\")\n",
        "\n",
        "print(\"\\nFolgende Spalten sind vom Typ 'object' oder 'category':\")\n",
        "for i, col in enumerate(kategorische_spalten):\n",
        "    print(f\"  {i+1}. {col}\")\n",
        "\n",
        "# Dropdown-Men√º und OK-Button erstellen\n",
        "ziel_dropdown = widgets.Dropdown(\n",
        "    options=kategorische_spalten,\n",
        "    description='Zielvariable:',\n",
        "    disabled=False,\n",
        ")\n",
        "\n",
        "ok_button = widgets.Button(\n",
        "    description='OK',\n",
        "    button_style='success',\n",
        "    tooltip='Ausgew√§hlte Spalte best√§tigen'\n",
        ")\n",
        "\n",
        "output_widget = widgets.Output()\n",
        "\n",
        "def on_ok_button_clicked(b):\n",
        "    with output_widget:\n",
        "        clear_output(wait=True)\n",
        "        try:\n",
        "            zielvariable_name = ziel_dropdown.value\n",
        "\n",
        "            y = df_temp[zielvariable_name]\n",
        "            X = df_temp.drop(columns=[zielvariable_name])\n",
        "\n",
        "            print(f\"\\n‚úÖ Zielvariable: '{y.name}' (Datentyp: {y.dtype})\")\n",
        "            print(f\"Anzahl der Features: {X.shape[1]}\")\n",
        "\n",
        "            # === 2. One-Hot-Encoding f√ºr Features ===\n",
        "            kategorische_spalten_in_X = X.select_dtypes(include=['object', 'category']).columns\n",
        "            if not kategorische_spalten_in_X.empty:\n",
        "                X = pd.get_dummies(X, columns=kategorische_spalten_in_X, drop_first=True)\n",
        "                print(\"\\n‚úÖ Kategorische Spalten in X wurden kodiert (One-Hot-Encoding).\")\n",
        "            else:\n",
        "                print(\"\\n‚ÑπÔ∏è Keine kategorischen Spalten in X gefunden.\")\n",
        "\n",
        "            # === 3. Zielvariable kodieren ===\n",
        "            if y.dtype == 'object' or y.dtype.name == 'category':\n",
        "                le = LabelEncoder()\n",
        "                y_encoded = le.fit_transform(y)\n",
        "                print(\"\\n‚úÖ Zielvariable in numerische Werte kodiert (LabelEncoder).\")\n",
        "                for i, class_name in enumerate(le.classes_):\n",
        "                    print(f\"  '{class_name}' -> {i}\")\n",
        "            else:\n",
        "                le = None\n",
        "                y_encoded = y\n",
        "                print(\"\\n‚ÑπÔ∏è Zielvariable bereits numerisch, keine Kodierung erforderlich.\")\n",
        "\n",
        "            # === 4. Konsistenzpr√ºfung vor Split ===\n",
        "            print(f\"\\n--- Konsistenzpr√ºfung vor Train/Test-Split ---\")\n",
        "            if X.shape[0] != y_encoded.shape[0]:\n",
        "                raise ValueError(f\"‚ùå Inkonsistente Anzahl Samples: X={X.shape[0]}, y={y_encoded.shape[0]}\")\n",
        "            else:\n",
        "                print(\"‚úÖ Shapes konsistent.\")\n",
        "\n",
        "            # === 5. Train/Test-Split ===\n",
        "            X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "            print(f\"\\nX_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
        "            print(f\"y_train: {y_train.shape}, y_test: {y_test.shape}\")\n",
        "\n",
        "            # === 6. Modelltraining und Bewertung (interaktiver Teil) ===\n",
        "            print(\"\\n--- üß† Modelle trainieren und bewerten ---\")\n",
        "            modelle = {\n",
        "                \"Logistische Regression\": LogisticRegression(max_iter=500),\n",
        "                \"KNN\": KNeighborsClassifier(),\n",
        "                \"Entscheidungsbaum\": DecisionTreeClassifier(),\n",
        "                \"Random Forest\": RandomForestClassifier(),\n",
        "                \"SVM\": SVC(probability=True),\n",
        "                \"Naive Bayes\": GaussianNB(),\n",
        "                \"Gradient Boosting\": GradientBoostingClassifier(),\n",
        "                \"AdaBoost\": AdaBoostClassifier(),\n",
        "                \"Neuronales Netz (MLP)\": MLPClassifier(max_iter=500),\n",
        "                \"Extra Trees\": ExtraTreesClassifier(),\n",
        "                \"Bagging\": BaggingClassifier(),\n",
        "                \"SGD\": SGDClassifier(loss=\"log_loss\"),\n",
        "                \"Ridge\": RidgeClassifier()\n",
        "            }\n",
        "\n",
        "            if has_xgb:\n",
        "                modelle[\"XGBoost\"] = XGBClassifier(eval_metric=\"mlogloss\", use_label_encoder=False)\n",
        "            if has_lgbm:\n",
        "                modelle[\"LightGBM\"] = LGBMClassifier(verbose=-1)\n",
        "            if has_catboost:\n",
        "                modelle[\"CatBoost\"] = CatBoostClassifier(verbose=0)\n",
        "\n",
        "            # Dropdown-Men√º f√ºr die Modellauswahl erstellen\n",
        "            modell_dropdown = widgets.Dropdown(\n",
        "                options=list(modelle.keys()),\n",
        "                description='Modell ausw√§hlen:',\n",
        "                disabled=False,\n",
        "                style={'description_width': 'initial'}\n",
        "            )\n",
        "            train_button = widgets.Button(\n",
        "                description='Modell trainieren',\n",
        "                button_style='primary',\n",
        "                tooltip='Ausgew√§hltes Modell trainieren und bewerten'\n",
        "            )\n",
        "            output_modell_widget = widgets.Output()\n",
        "\n",
        "            def on_train_button_clicked(b):\n",
        "                with output_modell_widget:\n",
        "                    clear_output(wait=True)\n",
        "                    try:\n",
        "                        selected_model_name = modell_dropdown.value\n",
        "                        model = modelle[selected_model_name]\n",
        "\n",
        "                        print(f\"Starte Training f√ºr: {selected_model_name}...\")\n",
        "\n",
        "                        # Cross-Validation zur Bewertung der Modellleistung\n",
        "                        scores = cross_val_score(model, X, y_encoded, cv=5, scoring='accuracy')\n",
        "                        print(f\"‚úÖ Cross-Validation (5-fold) abgeschlossen.\")\n",
        "                        print(f\"   CV-Genauigkeit: {np.mean(scores):.3f} (+/- {np.std(scores):.3f})\")\n",
        "\n",
        "                        # Modell auf dem gesamten Datensatz trainieren\n",
        "                        model.fit(X, y_encoded)\n",
        "                        print(f\"‚úÖ Training des Modells '{selected_model_name}' auf dem gesamten Datensatz abgeschlossen.\")\n",
        "\n",
        "                        # --- Visualisierung der Ergebnisse ---\n",
        "                        print(\"\\n--- üé® Visualisierung der Modellvorhersage (PCA) ---\")\n",
        "\n",
        "                        # Reduzieren Sie die Dimensionen mit PCA auf 2\n",
        "                        pca = PCA(n_components=2)\n",
        "                        X_pca = pca.fit_transform(X)\n",
        "\n",
        "                        # Erstellen der Vorhersagen f√ºr den gesamten Datensatz\n",
        "                        y_pred_all = model.predict(X)\n",
        "\n",
        "                        # Die tats√§chliche und die vorhergesagte Klasse in einem DataFrame zusammenfassen\n",
        "                        plot_df = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
        "                        plot_df['Echte_Klasse'] = y_encoded\n",
        "                        plot_df['Vorhergesagte_Klasse'] = y_pred_all\n",
        "\n",
        "                        # Labels f√ºr die Legende vorbereiten\n",
        "                        if le is not None:\n",
        "                            class_labels = le.classes_\n",
        "                        else:\n",
        "                            class_labels = np.unique(y_encoded)\n",
        "\n",
        "                        # === Scatter Plots erstellen ===\n",
        "                        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "                        # Plot 1: Tats√§chliche Klassen\n",
        "                        sns.scatterplot(\n",
        "                            x='PC1',\n",
        "                            y='PC2',\n",
        "                            hue='Echte_Klasse',\n",
        "                            data=plot_df,\n",
        "                            palette='viridis',\n",
        "                            legend='full',\n",
        "                            ax=ax1\n",
        "                        )\n",
        "                        ax1.set_title('Tats√§chliche Klassen (PCA 2D)')\n",
        "                        ax1.set_xlabel('Hauptkomponente 1')\n",
        "                        ax1.set_ylabel('Hauptkomponente 2')\n",
        "\n",
        "                        # Plot 2: Vorhergesagte Klassen\n",
        "                        sns.scatterplot(\n",
        "                            x='PC1',\n",
        "                            y='PC2',\n",
        "                            hue='Vorhergesagte_Klasse',\n",
        "                            data=plot_df,\n",
        "                            palette='viridis',\n",
        "                            legend='full',\n",
        "                            ax=ax2\n",
        "                        )\n",
        "                        ax2.set_title(f'Vorhergesagte Klassen von {selected_model_name} (PCA 2D)')\n",
        "                        ax2.set_xlabel('Hauptkomponente 1')\n",
        "                        ax2.set_ylabel('Hauptkomponente 2')\n",
        "\n",
        "                        # Legende anpassen\n",
        "                        handles1, labels1 = ax1.get_legend_handles_labels()\n",
        "                        handles2, labels2 = ax2.get_legend_handles_labels()\n",
        "\n",
        "                        if le is not None:\n",
        "                            labels_decoded = le.inverse_transform([int(float(l)) for l in labels1])\n",
        "                            ax1.legend(handles=handles1, labels=labels_decoded, title=\"Echte Klassen\")\n",
        "\n",
        "                            labels_decoded_pred = le.inverse_transform([int(float(l)) for l in labels2])\n",
        "                            ax2.legend(handles=handles2, labels=labels_decoded_pred, title=\"Vorhergesagte Klassen\")\n",
        "\n",
        "                        plt.tight_layout()\n",
        "                        plt.show()\n",
        "\n",
        "                        # --- Konfusionsmatrix ---\n",
        "                        y_pred_test = model.predict(X_test)\n",
        "                        if le is not None:\n",
        "                            display_labels = le.classes_\n",
        "                        else:\n",
        "                            display_labels = np.unique(np.concatenate((y_test, y_pred_test)))\n",
        "                        cm = confusion_matrix(y_test, y_pred_test, labels=np.arange(len(display_labels)))\n",
        "                        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=display_labels)\n",
        "                        disp.plot(cmap=\"Blues\", values_format=\"d\")\n",
        "                        plt.title(f\"Konfusionsmatrix ‚Äì {selected_model_name}\")\n",
        "                        plt.show()\n",
        "\n",
        "                        print(\"\\n--- üèÅ Analyse abgeschlossen ---\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"‚ùå Ein Fehler ist aufgetreten: {e}\")\n",
        "\n",
        "            train_button.on_click(on_train_button_clicked)\n",
        "\n",
        "            # Widgets anzeigen\n",
        "            print(\"Bitte w√§hlen Sie ein Modell aus und klicken Sie auf 'Modell trainieren'.\")\n",
        "            display(widgets.VBox([\n",
        "                modell_dropdown,\n",
        "                train_button,\n",
        "                output_modell_widget\n",
        "            ]))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Ein Fehler ist aufgetreten: {e}\")\n",
        "\n",
        "ok_button.on_click(on_ok_button_clicked)\n",
        "\n",
        "# Widgets anzeigen\n",
        "print(\"\\nBitte w√§hlen Sie die Zielvariable aus und klicken Sie auf OK.\")\n",
        "display(widgets.VBox([\n",
        "    ziel_dropdown,\n",
        "    ok_button,\n",
        "    output_widget\n",
        "]))"
      ],
      "metadata": {
        "id": "E67l2lTAbxvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML Un√ºberwachtes Lernen"
      ],
      "metadata": {
        "id": "bFpvrWQtcEfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================\n",
        "üìä Un√ºberwachtes Lernen mit ML\n",
        "# =====================================\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, SpectralClustering, Birch\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "import warnings\n",
        "import sys\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from ipywidgets import VBox, Dropdown, Button, Output\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ====== GLOBALE VARIABLEN F√úR DIE KOMMUNIKATION ZWISCHEN MODULEN ======\n",
        "X_scaled_df = None\n",
        "df_temp = None\n",
        "best_method = None\n",
        "solution_column_name = None\n",
        "all_methods_results = None\n",
        "\n",
        "def check_dependencies():\n",
        "    \"\"\"√úberpr√ºft, ob alle erforderlichen Bibliotheken installiert sind.\"\"\"\n",
        "    required_libraries = [\n",
        "        \"numpy\",\n",
        "        \"pandas\",\n",
        "        \"matplotlib\",\n",
        "        \"seaborn\",\n",
        "        \"sklearn\",\n",
        "        \"ipywidgets\",\n",
        "        \"scipy\"\n",
        "    ]\n",
        "    missing_libraries = []\n",
        "    for lib in required_libraries:\n",
        "        try:\n",
        "            __import__(lib)\n",
        "        except ImportError:\n",
        "            missing_libraries.append(lib)\n",
        "\n",
        "    if missing_libraries:\n",
        "        print(\"‚ùå Folgende Bibliotheken fehlen oder konnten nicht geladen werden:\")\n",
        "        for lib in missing_libraries:\n",
        "            print(f\"   - {lib}\")\n",
        "        print(\"\\nBitte installieren Sie die fehlenden Bibliotheken mit 'pip install [Bibliothek]'\")\n",
        "        sys.exit(\"Programm wird beendet, da nicht alle Abh√§ngigkeiten erf√ºllt sind.\")\n",
        "    else:\n",
        "        print(\"‚úÖ Alle erforderlichen Bibliotheken sind installiert.\")\n",
        "\n",
        "#============================================================\n",
        "def start_clustering_workflow(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Startet einen interaktiven Workflow f√ºr die un√ºberwachte Cluster-Analyse.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Das DataFrame, das f√ºr das Clustering verwendet werden soll.\n",
        "    \"\"\"\n",
        "    global X_scaled_df, df_temp, solution_column_name\n",
        "\n",
        "    print(' * * * * * * * * * *', '[DATEN PR√úFEN UND VORBEREITEN]', ' * * * * * * * * * *')\n",
        "    df_temp = df.copy()\n",
        "\n",
        "    # Automatische Erkennung von kategorialen Spalten, die f√ºr den Vergleich relevant sein k√∂nnten\n",
        "    kategorische_spalten = df_temp.select_dtypes(include=['object', 'category', 'int64']).columns.tolist()\n",
        "\n",
        "    if not kategorische_spalten:\n",
        "        print(\"‚ùå Keine kategorialen Spalten im DataFrame f√ºr den Vergleich gefunden.\")\n",
        "        return\n",
        "\n",
        "    def select_solution_column(b):\n",
        "        global solution_column_name, X_scaled_df\n",
        "        with output_pre:\n",
        "            clear_output()\n",
        "            solution_column_name = solution_dropdown.value\n",
        "            print(f\"‚û°Ô∏è L√∂sungsspalte '{solution_column_name}' ausgew√§hlt.\")\n",
        "\n",
        "            # Daten f√ºr Clustering vorbereiten\n",
        "            X = df_temp.drop(columns=[solution_column_name]).copy()\n",
        "\n",
        "            kategorische_spalten_clustering = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "            if kategorische_spalten_clustering:\n",
        "                X = pd.get_dummies(X, columns=kategorische_spalten_clustering, drop_first=True)\n",
        "                print(\"‚úÖ Kategorische Spalten wurden kodiert (One-Hot-Encoding).\")\n",
        "\n",
        "            numerische_spalten = X.select_dtypes(include=np.number).columns.tolist()\n",
        "            if not numerische_spalten:\n",
        "                print(\"‚ùå Keine numerischen Spalten f√ºr das Clustering im DataFrame gefunden!\")\n",
        "                return\n",
        "\n",
        "            scaler = StandardScaler()\n",
        "            X_scaled = scaler.fit_transform(X[numerische_spalten])\n",
        "            X_scaled_df = pd.DataFrame(X_scaled, columns=numerische_spalten, index=X.index)\n",
        "            print(\"‚úÖ Numerische Spalten wurden skaliert (StandardScaler).\")\n",
        "            print('*' * 50)\n",
        "\n",
        "            run_elbow_method()\n",
        "\n",
        "    output_pre = Output()\n",
        "    solution_dropdown = Dropdown(\n",
        "        options=kategorische_spalten,\n",
        "        description='L√∂sungsspalte w√§hlen:',\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "    pre_button = Button(description=\"Daten vorbereiten\")\n",
        "    pre_button.on_click(select_solution_column)\n",
        "\n",
        "    display(VBox([solution_dropdown, pre_button, output_pre]))\n",
        "#============================================================\n",
        "def run_elbow_method():\n",
        "    \"\"\"Zeigt die Ellbogen-Methode zur Bestimmung der optimalen Cluster-Anzahl.\"\"\"\n",
        "    global X_scaled_df\n",
        "    print(' * * * * * * * * * *', '[ELLBOGEN-METHODE]', ' * * * * * * * * * *')\n",
        "    sum_of_squared_distances = []\n",
        "    K_range = range(1, 11)\n",
        "    try:\n",
        "        for k in K_range:\n",
        "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "            kmeans.fit(X_scaled_df)\n",
        "            sum_of_squared_distances.append(kmeans.inertia_)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(K_range, sum_of_squared_distances, 'bx-')\n",
        "        plt.xlabel('Anzahl der Cluster (k)')\n",
        "        plt.ylabel('Summe der quadrierten Abst√§nde')\n",
        "        plt.title('Ellbogen-Methode (KMeans)')\n",
        "        plt.xticks(K_range)\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "        print(\"‚úÖ Ellbogen-Methode visualisiert. Suchen Sie nach der 'Biegung' im Diagramm.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Ellbogen-Methode konnte nicht ausgef√ºhrt werden: {e}\")\n",
        "    print('*' * 50)\n",
        "\n",
        "    show_clustering_widgets()\n",
        "#============================================================\n",
        "def show_clustering_widgets():\n",
        "    \"\"\"Zeigt Widgets f√ºr den Vergleich verschiedener Clustering-Methoden.\"\"\"\n",
        "    global X_scaled_df, all_methods_results, best_method\n",
        "    print(' * * * * * * * * * *', '[CLUSTERING-VERGLEICH]', ' * * * * * * * * * *')\n",
        "    k_dropdown = Dropdown(\n",
        "        options=list(range(2, 11)),\n",
        "        value=4,\n",
        "        description='Cluster k:',\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "    run_button = Button(description=\"Clustering vergleichen\")\n",
        "    output = Output()\n",
        "\n",
        "    def run_clustering(b):\n",
        "        global best_method, all_methods_results\n",
        "        with output:\n",
        "            clear_output()\n",
        "            k_optimal = k_dropdown.value\n",
        "            print(f\"‚û°Ô∏è Du hast k={k_optimal} ausgew√§hlt.\\n\")\n",
        "            results = []\n",
        "\n",
        "            # Dynamische Definition der K-basierten Clustering-Methoden\n",
        "            k_based_methods = {\n",
        "                \"KMeans\": KMeans(n_clusters=k_optimal, random_state=42, n_init=10),\n",
        "                \"GMM\": GaussianMixture(n_components=k_optimal, random_state=42),\n",
        "                \"Agglomerative\": AgglomerativeClustering(n_clusters=k_optimal),\n",
        "                \"SpectralClustering\": SpectralClustering(n_clusters=k_optimal, random_state=42, assign_labels='discretize'),\n",
        "                \"Birch\": Birch(n_clusters=k_optimal)\n",
        "            }\n",
        "\n",
        "            for name, model in k_based_methods.items():\n",
        "                try:\n",
        "                    labels = model.fit_predict(X_scaled_df)\n",
        "                    results.append({\n",
        "                        \"Methode\": name,\n",
        "                        \"Parameter\": f\"k={k_optimal}\",\n",
        "                        \"Silhouette\": silhouette_score(X_scaled_df, labels),\n",
        "                        \"Calinski-Harabasz\": calinski_harabasz_score(X_scaled_df, labels),\n",
        "                        \"Davies-Bouldin\": davies_bouldin_score(X_scaled_df, labels),\n",
        "                        \"Labels\": labels\n",
        "                    })\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Methode {name} konnte nicht ausgef√ºhrt werden: {e}\")\n",
        "\n",
        "            # DBSCAN mit automatischer eps-Sch√§tzung\n",
        "            try:\n",
        "                neigh = NearestNeighbors(n_neighbors=5)\n",
        "                nbrs = neigh.fit(X_scaled_df)\n",
        "                distances, _ = nbrs.kneighbors(X_scaled_df)\n",
        "                eps_est = np.percentile(distances[:, -1], 90)\n",
        "                dbscan = DBSCAN(eps=eps_est, min_samples=5)\n",
        "                labels = dbscan.fit_predict(X_scaled_df)\n",
        "                # √úberpr√ºfen, ob mehr als ein Cluster gefunden wurde\n",
        "                if len(set(labels)) > 1:\n",
        "                    results.append({\n",
        "                        \"Methode\": \"DBSCAN\",\n",
        "                        \"Parameter\": f\"eps={eps_est:.2f}\",\n",
        "                        \"Silhouette\": silhouette_score(X_scaled_df, labels),\n",
        "                        \"Calinski-Harabasz\": calinski_harabasz_score(X_scaled_df, labels),\n",
        "                        \"Davies-Bouldin\": davies_bouldin_score(X_scaled_df, labels),\n",
        "                        \"Labels\": labels\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Methode DBSCAN konnte nicht ausgef√ºhrt werden: {e}\")\n",
        "\n",
        "            if not results:\n",
        "                print(\"Keine Cluster-Methoden konnten erfolgreich ausgef√ºhrt werden.\")\n",
        "                return\n",
        "\n",
        "            all_methods_results = {res['Methode']: res for res in results}\n",
        "\n",
        "            results_df = pd.DataFrame(results).drop(columns=\"Labels\")\n",
        "            display(results_df.sort_values(by=\"Silhouette\", ascending=False))\n",
        "\n",
        "            best_method = max(results, key=lambda x: x[\"Silhouette\"])\n",
        "            print(f\"\\nüèÜ Beste Methode: {best_method['Methode']} ({best_method['Parameter']})\")\n",
        "\n",
        "            print('*' * 50)\n",
        "            show_comparison_widgets()\n",
        "\n",
        "    display(VBox([k_dropdown, run_button, output]))\n",
        "    run_button.on_click(run_clustering)\n",
        "#============================================================\n",
        "def show_comparison_widgets():\n",
        "    \"\"\"Zeigt Widgets f√ºr den Vergleich der Cluster mit den Originaldaten.\"\"\"\n",
        "    global df_temp, all_methods_results, solution_column_name\n",
        "    print(' * * * * * * * * * *', '[VISUALISIERUNG UND VERGLEICH]', ' * * * * * * * * * *')\n",
        "\n",
        "    if solution_column_name is None or all_methods_results is None:\n",
        "        print(\"‚ùå Keine Daten verf√ºgbar. Bitte f√ºhren Sie die vorherigen Schritte aus.\")\n",
        "        return\n",
        "\n",
        "    method_dropdown = Dropdown(\n",
        "        options=list(all_methods_results.keys()),\n",
        "        value=best_method['Methode'] if best_method else list(all_methods_results.keys())[0],\n",
        "        description='Methode w√§hlen:',\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "    obj_col_dropdown = Dropdown(\n",
        "        options=[solution_column_name],\n",
        "        description='Vergleichs-Spalte:',\n",
        "        disabled=True,\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "    show_plots_button = Button(description=\"Plots und Vergleich anzeigen\")\n",
        "    output_compare = Output()\n",
        "\n",
        "    def run_object_comparison(b):\n",
        "        with output_compare:\n",
        "            clear_output(wait=True)\n",
        "            selected_method_name = method_dropdown.value\n",
        "            selected_method = all_methods_results[selected_method_name]\n",
        "\n",
        "            print(f\"‚û°Ô∏è Analysiere Ergebnisse f√ºr Methode: {selected_method['Methode']} ({selected_method['Parameter']})\")\n",
        "\n",
        "            # PCA-Visualisierung der ausgew√§hlten Methode\n",
        "            pca = PCA(n_components=2)\n",
        "            pcs = pca.fit_transform(X_scaled_df)\n",
        "\n",
        "            labels_to_plot = selected_method['Labels']\n",
        "            if selected_method['Methode'] == 'DBSCAN':\n",
        "                mask = labels_to_plot != -1\n",
        "                pcs = pcs[mask]\n",
        "                labels_to_plot = labels_to_plot[mask]\n",
        "\n",
        "                if len(set(labels_to_plot)) == 0:\n",
        "                    print(\"‚ö†Ô∏è DBSCAN hat keine Cluster gefunden (nur Rauschen). Visualisierung nicht m√∂glich.\")\n",
        "                else:\n",
        "                    plt.figure(figsize=(10, 8))\n",
        "                    sns.scatterplot(x=pcs[:, 0], y=pcs[:, 1], hue=labels_to_plot, palette='viridis', legend='full', alpha=0.7)\n",
        "                    plt.title(f\"Cluster-Visualisierung ({selected_method['Methode']})\")\n",
        "                    plt.xlabel(\"PC1\")\n",
        "                    plt.ylabel(\"PC2\")\n",
        "                    plt.show()\n",
        "                    plt.close()\n",
        "            else:\n",
        "                plt.figure(figsize=(10, 8))\n",
        "                sns.scatterplot(x=pcs[:, 0], y=pcs[:, 1], hue=labels_to_plot, palette='viridis', legend='full', alpha=0.7)\n",
        "                plt.title(f\"Cluster-Visualisierung ({selected_method['Methode']})\")\n",
        "                plt.xlabel(\"PC1\")\n",
        "                plt.ylabel(\"PC2\")\n",
        "                plt.show()\n",
        "                plt.close()\n",
        "\n",
        "            # Vergleich mit Originalspalte\n",
        "            df_compare = df_temp.copy()\n",
        "            df_compare['Cluster'] = selected_method['Labels']\n",
        "\n",
        "            contingency = pd.crosstab(df_compare[solution_column_name], df_compare['Cluster'])\n",
        "            print(\"\\n--- Anzahl der Objekte pro Kategorie und Cluster ---\")\n",
        "            display(contingency)\n",
        "\n",
        "            cost_matrix = -contingency.values\n",
        "            row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "            cluster_mapping = {contingency.index[r]: contingency.columns[c] for r, c in zip(row_ind, col_ind)}\n",
        "\n",
        "            print(\"\\n‚úÖ Eindeutige Kategorie ‚Üí Cluster Zuordnung:\")\n",
        "            for cat, clus in cluster_mapping.items():\n",
        "                print(f\"  {cat} -> Cluster {clus}\")\n",
        "\n",
        "            df_compare['Predominant_Cluster'] = df_compare[solution_column_name].map(cluster_mapping)\n",
        "            df_compare['Abweichung'] = df_compare['Cluster'] != df_compare['Predominant_Cluster']\n",
        "            n_abw = df_compare['Abweichung'].sum()\n",
        "            print(f\"\\n‚ùå Anzahl der Abweichungen vom dominanten Cluster: {n_abw}\")\n",
        "\n",
        "            if n_abw > 0:\n",
        "                print(\"\\n--- Objekte mit Abweichung ---\")\n",
        "                display(df_compare[df_compare['Abweichung']].head())\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.heatmap(contingency, annot=True, fmt=\"d\", cmap=\"viridis\")\n",
        "            plt.title(f\"Cluster-Verteilung f√ºr '{solution_column_name}'\")\n",
        "            plt.ylabel(solution_column_name)\n",
        "            plt.xlabel(\"Cluster\")\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "            print('*' * 50)\n",
        "            create_temp_clear_ml()\n",
        "\n",
        "    display(VBox([method_dropdown, obj_col_dropdown, show_plots_button, output_compare]))\n",
        "    show_plots_button.on_click(run_object_comparison)\n",
        "#============================================================\n",
        "def create_temp_clear_ml():\n",
        "    \"\"\"Erstellt die finale Spalte TEMP_Clear_ML f√ºr die Machine Learning Vorverarbeitung.\"\"\"\n",
        "    global df_temp, best_method, solution_column_name\n",
        "    print(' * * * * * * * * * *', '[TEMP_Clear_ML ERSTELLEN]', ' * * * * * * * * * *')\n",
        "\n",
        "    ml_col_name = \"TEMP_Clear_ML\"\n",
        "\n",
        "    df_with_ml = df_temp.copy()\n",
        "    df_with_ml['Cluster'] = best_method['Labels']\n",
        "\n",
        "    comparison = pd.crosstab(df_with_ml[solution_column_name], df_with_ml['Cluster'])\n",
        "    cluster_mapping = {}\n",
        "    for cluster in comparison.columns:\n",
        "        assigned_category = comparison[cluster].idxmax()\n",
        "        cluster_mapping[cluster] = assigned_category\n",
        "\n",
        "    df_with_ml[ml_col_name] = df_with_ml.apply(\n",
        "        lambda row: cluster_mapping[row['Cluster']] if cluster_mapping[row['Cluster']] == row[solution_column_name] else False,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    print(f\"‚úÖ Neue Spalte '{ml_col_name}' erstellt.\")\n",
        "    display(df_with_ml[[solution_column_name, 'Cluster', ml_col_name]].head())\n",
        "\n",
        "    n_false = (df_with_ml[ml_col_name] == False).sum()\n",
        "    print(f\"\\n‚ùå Anzahl der nicht √ºbereinstimmenden Eintr√§ge: {n_false}\")\n",
        "    print('*' * 50)\n",
        "\n",
        "#============================================================\n",
        "#                    Skript starten\n",
        "#============================================================\n",
        "check_dependencies()\n",
        "if 'df' in locals():\n",
        "    start_clustering_workflow(df)\n",
        "else:\n",
        "    print(\"‚ùå DataFrame 'df' nicht gefunden. Bitte stellen Sie sicher, dass es geladen ist.\")\n",
        "\n",
        "# ============================================================\n",
        "#       Neue Funktion: Beste Methode finden\n",
        "# ============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def finde_beste_methode(df: pd.DataFrame, comparison_col: str, n_clusters: int) -> dict:\n",
        "    \"\"\"\n",
        "    Vergleicht verschiedene Clustering-Methoden und w√§hlt diejenige aus,\n",
        "    die die h√∂chste Genauigkeit (Trefferquote) im Vergleich zur\n",
        "    ausgew√§hlten Spalte aufweist.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Das Original-DataFrame.\n",
        "        comparison_col (str): Der Name der Spalte mit der L√∂sung.\n",
        "        n_clusters (int): Die Anzahl der zu erwartenden Cluster.\n",
        "\n",
        "    Returns:\n",
        "        dict: Ein W√∂rterbuch mit der besten Methode und deren Ergebnissen.\n",
        "    \"\"\"\n",
        "    df_temp = df.copy()\n",
        "\n",
        "    # Sicherstellen, dass die Vergleichsspalte eine Kategorie ist\n",
        "    df_temp['category_labels'] = pd.Categorical(df_temp[comparison_col]).codes\n",
        "\n",
        "    # Numerische Features f√ºr das Clustering extrahieren und skalieren\n",
        "    numeric_features = df.select_dtypes(include=['number']).columns.tolist()\n",
        "    X = df[numeric_features]\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    methods = {\n",
        "        'K-Means': KMeans(n_clusters=n_clusters, random_state=42, n_init='auto'),\n",
        "        'Agglomerative Clustering': AgglomerativeClustering(n_clusters=n_clusters)\n",
        "    }\n",
        "\n",
        "    best_accuracy = -1\n",
        "    best_method = {}\n",
        "\n",
        "    # Alle Methoden vergleichen\n",
        "    for name, method in methods.items():\n",
        "        labels = method.fit_predict(X_scaled)\n",
        "\n",
        "        # Sicherstellen, dass die Anzahl der Cluster der erwarteten entspricht\n",
        "        if len(np.unique(labels)) != n_clusters:\n",
        "            continue\n",
        "\n",
        "        # Optimales Mapping finden\n",
        "        contingency = pd.crosstab(df_temp['category_labels'], labels)\n",
        "        cost_matrix = -contingency.values\n",
        "        row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "\n",
        "        # Trefferquote (Accuracy) berechnen\n",
        "        accuracy = contingency.values[row_ind, col_ind].sum() / contingency.sum().sum()\n",
        "\n",
        "        # Beste Methode aktualisieren\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            best_method = {\n",
        "                'Name': name,\n",
        "                'Labels': labels,\n",
        "                'Accuracy': accuracy\n",
        "            }\n",
        "\n",
        "    return best_method\n",
        "\n",
        "# ============================================================\n",
        "#                    Cluster-Ergebnisse\n",
        "# ============================================================\n",
        "\n",
        "def vergleiche_cluster_mit_originaldaten(df: pd.DataFrame, best_method: dict):\n",
        "    \"\"\"\n",
        "    Erstellt interaktive Widgets, um die Cluster-Ergebnisse mit einer\n",
        "    ausgew√§hlten Original-Kategoriespalte zu vergleichen und zu visualisieren.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Das Original-DataFrame.\n",
        "        best_method (dict): Das W√∂rterbuch mit den Ergebnissen der besten Clustering-Methode.\n",
        "    \"\"\"\n",
        "    object_columns = df.select_dtypes(include=['object', 'category', 'int64']).columns.tolist()\n",
        "\n",
        "    if not object_columns:\n",
        "        print(\"‚ùå Keine geeigneten kategorialen Spalten im DataFrame f√ºr den Vergleich vorhanden!\")\n",
        "        return\n",
        "\n",
        "    # Anzeige der besten Methode basierend auf der h√∂chsten Genauigkeit\n",
        "    print(f\"üèÜ Beste Methode in L√∂sungsspalte zu Clustering: {best_method['Name']} (Accuracy: {best_method['Accuracy']:.2%})\")\n",
        "\n",
        "    obj_col_dropdown = widgets.Dropdown(\n",
        "        options=object_columns,\n",
        "        description='Vergleichs-Spalte:',\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "    compare_button = widgets.Button(description=\"Vergleich starten\")\n",
        "    output_compare = widgets.Output()\n",
        "\n",
        "    def run_object_comparison(b):\n",
        "        with output_compare:\n",
        "            clear_output(wait=True)\n",
        "            col = obj_col_dropdown.value\n",
        "            print(f\"‚û°Ô∏è Gew√§hlte Spalte f√ºr Vergleich: {col}\")\n",
        "\n",
        "            # Gesamt-DF mit Cluster-Spalte\n",
        "            df_compare = df.copy()\n",
        "            df_compare['Cluster'] = best_method['Labels']\n",
        "\n",
        "            # Kontingenztabelle\n",
        "            contingency = pd.crosstab(df_compare[col], df_compare['Cluster'])\n",
        "            print(\"\\n--- Anzahl der Objekte pro Kategorie und Cluster ---\")\n",
        "            display(contingency)\n",
        "\n",
        "            # --- Optimale 1:1 Zuordnung Kategorie ‚Üí Cluster ---\n",
        "            cost_matrix = -contingency.values\n",
        "            row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
        "            cluster_mapping = {contingency.index[r]: contingency.columns[c] for r, c in zip(row_ind, col_ind)}\n",
        "\n",
        "            print(\"\\n‚úÖ Eindeutige Kategorie ‚Üí Cluster Zuordnung:\")\n",
        "            for cat, clus in cluster_mapping.items():\n",
        "                print(f\"  {cat} -> Cluster {clus}\")\n",
        "\n",
        "            # Berechne Abweichungen\n",
        "            df_compare['Predominant_Cluster'] = df_compare[col].map(cluster_mapping)\n",
        "            df_compare['Abweichung'] = df_compare['Cluster'] != df_compare['Predominant_Cluster']\n",
        "            n_abw = df_compare['Abweichung'].sum()\n",
        "            print(f\"\\n‚ùå Anzahl der Abweichungen vom dominanten Cluster: {n_abw}\")\n",
        "            if n_abw > 0:\n",
        "                print(\"\\n--- Objekte mit Abweichung ---\")\n",
        "                display(df_compare[df_compare['Abweichung']].head())\n",
        "\n",
        "            # --- Heatmap ---\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            sns.heatmap(contingency, annot=True, fmt=\"d\", cmap=\"viridis\")\n",
        "            plt.title(f\"Cluster-Verteilung f√ºr '{col}'\")\n",
        "            plt.ylabel(col)\n",
        "            plt.xlabel(\"Cluster\")\n",
        "            plt.show()\n",
        "\n",
        "            # --- Balkenplot Abweichungen pro Kategorie ---\n",
        "            abw_per_cat = df_compare.groupby(col)['Abweichung'].sum()\n",
        "            plt.figure(figsize=(8,4))\n",
        "            sns.barplot(x=abw_per_cat.index, y=abw_per_cat.values, palette=\"coolwarm\")\n",
        "            plt.title(f\"Abweichungen vom dominanten Cluster pro Kategorie\")\n",
        "            plt.ylabel(\"Anzahl Abweichungen\")\n",
        "            plt.xlabel(col)\n",
        "            plt.show()\n",
        "\n",
        "    # Anzeige der Widgets\n",
        "    display(widgets.VBox([obj_col_dropdown, compare_button, output_compare]))\n",
        "    compare_button.on_click(run_object_comparison)\n",
        "\n",
        "#===================================================\n",
        "\n",
        "# --- ML-Zuordnung in df mit Kategorie-Labels und False f√ºr Abweichungen mit Button ---\n",
        "object_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "if not object_columns:\n",
        "    print(\"‚ùå Keine Object-Spalten im DataFrame vorhanden!\")\n",
        "else:\n",
        "    obj_col_dropdown = widgets.Dropdown(\n",
        "        options=object_columns,\n",
        "        description='Vergleichs-Spalte:',\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    ml_button = widgets.Button(description=\"ML-Zuordnung erstellen\")\n",
        "    output_ml = widgets.Output()\n",
        "\n",
        "    def run_ml_assignment(b):\n",
        "        with output_ml:\n",
        "            clear_output(wait=True)\n",
        "            col = obj_col_dropdown.value\n",
        "            print(f\"‚û°Ô∏è Gew√§hlte Spalte f√ºr ML-Zuordnung: {col}\")\n",
        "\n",
        "            if 'best_method' not in globals():\n",
        "                print(\"‚ùå Bitte zuerst Clustering ausf√ºhren!\")\n",
        "                return\n",
        "\n",
        "            ml_col_name = f\"{col}_ML\"\n",
        "\n",
        "            # Gesamt-DF mit Cluster-Spalte\n",
        "            df_with_ml = df.copy()\n",
        "            df_with_ml['Cluster'] = best_method['Labels']\n",
        "\n",
        "            # Gruppierung Original-Spalte vs. Cluster\n",
        "            comparison = pd.crosstab(df_with_ml[col], df_with_ml['Cluster'])\n",
        "\n",
        "            # Eindeutige Zuordnung Cluster -> Original-Kategorie\n",
        "            cluster_mapping = {}\n",
        "            for cluster in comparison.columns:\n",
        "                assigned_category = comparison[cluster].idxmax()\n",
        "                cluster_mapping[cluster] = assigned_category\n",
        "\n",
        "            # Neue Spalte einf√ºgen: Original-Kategorie wenn Treffer, sonst False\n",
        "            df_with_ml[ml_col_name] = df_with_ml.apply(\n",
        "                lambda row: cluster_mapping[row['Cluster']] if cluster_mapping[row['Cluster']] == row[col] else False,\n",
        "                axis=1\n",
        "            )\n",
        "\n",
        "            # Ausgabe\n",
        "            print(f\"‚úÖ Neue Spalte '{ml_col_name}' erstellt:\")\n",
        "            display(df_with_ml[[col, 'Cluster', ml_col_name]].head(50))\n",
        "\n",
        "            # Anzahl der nicht √ºbereinstimmenden Eintr√§ge\n",
        "            n_false = (df_with_ml[ml_col_name] == False).sum()\n",
        "            print(f\"\\n‚ùå Anzahl der nicht √ºbereinstimmenden Eintr√§ge: {n_false}\")\n",
        "            df_with_ml.to_csv(\"df_with_ml.csv\", index=False)\n",
        "\n",
        "\n",
        "    # Anzeige der Widgets\n",
        "    display(widgets.VBox([obj_col_dropdown, ml_button, output_ml]))\n",
        "    ml_button.on_click(run_ml_assignment)"
      ],
      "metadata": {
        "id": "efsaLbXBcO9r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}